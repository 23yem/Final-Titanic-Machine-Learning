{"cells":[{"cell_type":"code","execution_count":224,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-08T16:54:37.331383Z","iopub.status.busy":"2023-12-08T16:54:37.330932Z","iopub.status.idle":"2023-12-08T16:54:37.342250Z","shell.execute_reply":"2023-12-08T16:54:37.341093Z","shell.execute_reply.started":"2023-12-08T16:54:37.331345Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n"]},{"cell_type":"code","execution_count":225,"metadata":{},"outputs":[],"source":["# import tensorflow as tf\n","# print(tf.__version__)\n","\n","# import kerastuner as kt\n","# print(kt.__version__)\n","\n","# import keras_tuner as kt2\n","# print(kt2.__version__)\n","\n","# import platform\n","# print(platform.python_version())\n","\n","# import numpy as np\n","# print(np.__version__)\n","\n","# import pandas as pd\n","# print(pd.__version__)\n","\n","# import sklearn\n","# print(sklearn.__version__)\n","\n","# import sklearn\n","# print(sklearn.__path__)"]},{"cell_type":"markdown","metadata":{},"source":["# Set Global random seed to make sure we can replicate any model that we create (no randomness)"]},{"cell_type":"code","execution_count":226,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.345037Z","iopub.status.busy":"2023-12-08T16:54:37.344445Z","iopub.status.idle":"2023-12-08T16:54:37.359350Z","shell.execute_reply":"2023-12-08T16:54:37.357656Z","shell.execute_reply.started":"2023-12-08T16:54:37.344996Z"},"trusted":true},"outputs":[],"source":["import random\n","import tensorflow as tf\n","\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["# **Load the Data from the Database**"]},{"cell_type":"code","execution_count":227,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.367577Z","iopub.status.busy":"2023-12-08T16:54:37.367107Z","iopub.status.idle":"2023-12-08T16:54:37.396537Z","shell.execute_reply":"2023-12-08T16:54:37.395481Z","shell.execute_reply.started":"2023-12-08T16:54:37.367545Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  "]},"execution_count":227,"metadata":{},"output_type":"execute_result"}],"source":["#Remember to eventually clean up the wrong or missing values\n","train_data = pd.read_csv(\"train.csv\")\n","\n","train_data.head()"]},{"cell_type":"code","execution_count":228,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.397932Z","iopub.status.busy":"2023-12-08T16:54:37.397600Z","iopub.status.idle":"2023-12-08T16:54:37.418582Z","shell.execute_reply":"2023-12-08T16:54:37.417408Z","shell.execute_reply.started":"2023-12-08T16:54:37.397901Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>892</td>\n","      <td>3</td>\n","      <td>Kelly, Mr. James</td>\n","      <td>male</td>\n","      <td>34.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>330911</td>\n","      <td>7.8292</td>\n","      <td>NaN</td>\n","      <td>Q</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>893</td>\n","      <td>3</td>\n","      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n","      <td>female</td>\n","      <td>47.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>363272</td>\n","      <td>7.0000</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>894</td>\n","      <td>2</td>\n","      <td>Myles, Mr. Thomas Francis</td>\n","      <td>male</td>\n","      <td>62.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>240276</td>\n","      <td>9.6875</td>\n","      <td>NaN</td>\n","      <td>Q</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>895</td>\n","      <td>3</td>\n","      <td>Wirz, Mr. Albert</td>\n","      <td>male</td>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>315154</td>\n","      <td>8.6625</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>896</td>\n","      <td>3</td>\n","      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n","      <td>female</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3101298</td>\n","      <td>12.2875</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Pclass                                          Name     Sex  \\\n","0          892       3                              Kelly, Mr. James    male   \n","1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n","2          894       2                     Myles, Mr. Thomas Francis    male   \n","3          895       3                              Wirz, Mr. Albert    male   \n","4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n","\n","    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n","0  34.5      0      0   330911   7.8292   NaN        Q  \n","1  47.0      1      0   363272   7.0000   NaN        S  \n","2  62.0      0      0   240276   9.6875   NaN        Q  \n","3  27.0      0      0   315154   8.6625   NaN        S  \n","4  22.0      1      1  3101298  12.2875   NaN        S  "]},"execution_count":228,"metadata":{},"output_type":"execute_result"}],"source":["#Remember to eventually clean up the wrong or missing values\n","test_data = pd.read_csv(\"test.csv\")\n","\n","test_data.head()"]},{"cell_type":"code","execution_count":229,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.421938Z","iopub.status.busy":"2023-12-08T16:54:37.420746Z","iopub.status.idle":"2023-12-08T16:54:37.428795Z","shell.execute_reply":"2023-12-08T16:54:37.427335Z","shell.execute_reply.started":"2023-12-08T16:54:37.421904Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n","       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n","      dtype='object')\n","(891, 12)\n"]}],"source":["print(train_data.columns)\n","print(train_data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Choosing features #\n"]},{"cell_type":"code","execution_count":230,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.430711Z","iopub.status.busy":"2023-12-08T16:54:37.430372Z","iopub.status.idle":"2023-12-08T16:54:37.452427Z","shell.execute_reply":"2023-12-08T16:54:37.451542Z","shell.execute_reply.started":"2023-12-08T16:54:37.430681Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pclass</th>\n","      <th>Fare</th>\n","      <th>Sex_female</th>\n","      <th>Sex_male</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>7.2500</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>71.2833</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>7.9250</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>53.1000</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>8.0500</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pclass     Fare  Sex_female  Sex_male\n","0       3   7.2500       False      True\n","1       1  71.2833        True     False\n","2       3   7.9250        True     False\n","3       1  53.1000        True     False\n","4       3   8.0500       False      True"]},"execution_count":230,"metadata":{},"output_type":"execute_result"}],"source":["features = [\"Pclass\", \"Sex\", \"Fare\"]\n","\n","X_train = pd.get_dummies(train_data[features])\n","\n","X_train.head()\n","#print(X_train.shape)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Set the target/label (y) values\n"]},{"cell_type":"code","execution_count":231,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.454688Z","iopub.status.busy":"2023-12-08T16:54:37.453671Z","iopub.status.idle":"2023-12-08T16:54:37.462318Z","shell.execute_reply":"2023-12-08T16:54:37.461084Z","shell.execute_reply.started":"2023-12-08T16:54:37.454632Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0    0\n","1    1\n","2    1\n","3    1\n","4    0\n","Name: Survived, dtype: int64"]},"execution_count":231,"metadata":{},"output_type":"execute_result"}],"source":["#Y_train = pd.get_dummies(train_data[\"Survived\"])\n","Y_train = train_data[\"Survived\"]\n","\n","Y_train.head()\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Split up train data into Train-test using scikit-learn"]},{"cell_type":"code","execution_count":232,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.464271Z","iopub.status.busy":"2023-12-08T16:54:37.463420Z","iopub.status.idle":"2023-12-08T16:54:37.479285Z","shell.execute_reply":"2023-12-08T16:54:37.477561Z","shell.execute_reply.started":"2023-12-08T16:54:37.464239Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["445    1\n","650    0\n","172    1\n","450    0\n","314    0\n","Name: Survived, dtype: int64\n","709    1\n","439    0\n","840    0\n","720    1\n","39     1\n","Name: Survived, dtype: int64\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","#Split data into a train/validation and test set\n","X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 42)\n","\n","#print(X_train.head())\n","#print(Y_train.head())\n","print(Y_train.head())\n","\n","print(Y_test.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["spliting the data into a train-test split will allow us to test our model and so it will allow us to tune our model before submitting on Kaggle. We can also use the train-validation-test split here"]},{"cell_type":"markdown","metadata":{},"source":["# Making the Neural Network Model using Scikit Learn\n"]},{"cell_type":"code","execution_count":233,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.481985Z","iopub.status.busy":"2023-12-08T16:54:37.480548Z","iopub.status.idle":"2023-12-08T16:54:37.489339Z","shell.execute_reply":"2023-12-08T16:54:37.487834Z","shell.execute_reply.started":"2023-12-08T16:54:37.481921Z"},"trusted":true},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Commented out the code so that the model won't run everytime\n","\n","# model = MLPClassifier(hidden_layer_sizes=(5, 5), activation='relu', solver='adam', max_iter=1000, random_state=42)\n","\n","# model.fit(X_train, Y_train)\n","\n","# y_pred = model.predict(X_test)\n","\n","# print(f\"Accuracy: {accuracy_score(Y_test, y_pred):.2f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["**Here I am using the MLPClassifier since this is a classification problem. The output of the model will be \"Survive\" or \"not survive.\" If my model was supposed to predict a numerical value, I would be using something like MLPRegressor instead**"]},{"cell_type":"markdown","metadata":{},"source":["# Tried making a neural network using Tensor Flow but decided to just use Scikit learn"]},{"cell_type":"code","execution_count":234,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.494249Z","iopub.status.busy":"2023-12-08T16:54:37.493526Z","iopub.status.idle":"2023-12-08T16:54:37.503223Z","shell.execute_reply":"2023-12-08T16:54:37.501422Z","shell.execute_reply.started":"2023-12-08T16:54:37.494208Z"},"trusted":true},"outputs":[],"source":["# import tensorflow as tf\n","\n","\n","# #Compile the model\n","# model = tf.keras.models.Sequential([\n","#     tf.keras.layers.Dense(64, activation = \"relu\", input_shape = (687,)),\n","#     tf.keras.layers.Dense(32, activation = \"relu\"),\n","#     tf.keras.layers.Dense(16, activation = \"relu\"),\n","#     tf.keras.layers.Dense(1, activation = \"sigmoid\")\n","# ])\n","# model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n","\n","\n","# #Train the model\n","\n","# #X_train = tf.convert_to_tensor(X_train)\n","# #Y_train = tf.convert_to_tensor(Y_train)\n","# model.fit(X_train, Y_train, epochs = 10)\n","\n","# #Evaluate the model \n","\n","# #X_test = tf.convert_to_tensor(X_test)\n","# #Y_test = tf.convert_to_tensor(Y_test)\n","# loss, accuracy = model.evaluate(X_test, Y_test)\n","# print(\"Accuracy: \", accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["# Testing model on real test set and Submitting Model (Accuracy: 0.755)\n"]},{"cell_type":"code","execution_count":235,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.505207Z","iopub.status.busy":"2023-12-08T16:54:37.504735Z","iopub.status.idle":"2023-12-08T16:54:37.538079Z","shell.execute_reply":"2023-12-08T16:54:37.536919Z","shell.execute_reply.started":"2023-12-08T16:54:37.505166Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['Pclass', 'Fare', 'Sex_female', 'Sex_male', 'PassengerId'], dtype='object')\n"]}],"source":["test_data = pd.read_csv(\"test.csv\")\n","features = [\"Pclass\", \"Sex\", \"Fare\"]\n","test_X = pd.get_dummies(test_data[features])\n","#test_X = test_X.dropna()\n","#print(test_X[\"Sex_female\"])\n","test_X['Fare'].fillna(test_X['Fare'].median(), inplace=True)\n","\n","# Impute categorical columns with the mode\n","for column in ['Pclass']:\n","    test_X[column].fillna(test_X[column].mode()[0], inplace=True)\n","\n","for column in [\"Sex_female\"]:\n","    test_X[column].fillna(False, inplace=True)\n","    \n","for column in ['Sex_male']:\n","    test_X[column].fillna(True, inplace=True)\n","  \n","  \n","# print(X_test)\n","\n","#model.fit(X_train, Y_train)\n","\n","# predictions = model.predict(test_X)\n","\n","pass_id = test_data['PassengerId']\n","\n","test_data = test_data.drop('PassengerId', axis=1)\n","test_data = pd.get_dummies(test_data[features])\n","\n","test_data[\"PassengerId\"] = pass_id\n","\n","test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)\n","\n","print(test_data.columns)\n","\n","# Impute categorical columns with the mode\n","for column in ['Pclass']:\n","    test_data[column].fillna(test_data[column].mode()[0], inplace=True)\n","\n","for column in ['Sex_female']:\n","    test_data[column].fillna(False, inplace=True)\n","    \n","for column in ['Sex_male']:\n","    test_data[column].fillna(True, inplace=True)\n","    \n","# Commented out the code so that the model won't run everytime\n","\n","# print(test_X.shape)\n","# print(predictions.shape)\n","# print(test_data.shape)\n","\n","\n","# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")"]},{"cell_type":"markdown","metadata":{},"source":["**The accuracy of the model was 0.755**"]},{"cell_type":"markdown","metadata":{},"source":["# After first submission, I will now try and edit the neural network to improve the accurary\n"]},{"cell_type":"code","execution_count":236,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.539737Z","iopub.status.busy":"2023-12-08T16:54:37.539404Z","iopub.status.idle":"2023-12-08T16:54:37.544492Z","shell.execute_reply":"2023-12-08T16:54:37.543385Z","shell.execute_reply.started":"2023-12-08T16:54:37.539707Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","# model = MLPClassifier(hidden_layer_sizes=(14, 4), activation='relu', solver='lbfgs', max_iter=1000, random_state=42)\n","\n","# model.fit(X_train, Y_train)\n","\n","# y_pred = model.predict(X_test)\n","\n","# print(f\"Accuracy: {accuracy_score(Y_test, y_pred):.2f}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**I edited  the hidden layer sizes**\n"]},{"cell_type":"markdown","metadata":{},"source":["# Second submission (accuracy: 0.76)\n"]},{"cell_type":"code","execution_count":237,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.545897Z","iopub.status.busy":"2023-12-08T16:54:37.545604Z","iopub.status.idle":"2023-12-08T16:54:37.569193Z","shell.execute_reply":"2023-12-08T16:54:37.568374Z","shell.execute_reply.started":"2023-12-08T16:54:37.545869Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["test_data = pd.read_csv(\"test.csv\")\n","features = [\"Pclass\", \"Sex\", \"Fare\"]\n","test_X = pd.get_dummies(test_data[features])\n","\n","test_X['Fare'].fillna(test_X['Fare'].median(), inplace=True)\n","\n","# Impute categorical columns with the mode\n","for column in ['Pclass']:\n","    test_X[column].fillna(test_X[column].mode()[0], inplace=True)\n","\n","for column in [\"Sex_female\"]:\n","    test_X[column].fillna(False, inplace=True)\n","    \n","for column in ['Sex_male']:\n","    test_X[column].fillna(True, inplace=True)\n","\n","print(\"done\")\n","\n","\n","# Commented out the code so that the model won't run everytime\n","\n","# predictions2 = model.predict(test_X)\n","\n","# pass_id = test_data['PassengerId']\n","\n","# test_data = test_data.drop('PassengerId', axis=1)\n","# test_data = pd.get_dummies(test_data[features])\n","\n","# test_data[\"PassengerId\"] = pass_id\n","\n","# print(test_data.columns)\n","\n","# print(test_X.shape)\n","# print(predictions2.shape)\n","# print(test_data.shape)\n","\n","# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")"]},{"cell_type":"markdown","metadata":{},"source":["**Here, from the code above, I went from a 0.755 to a 0.76 by editing the hidden layer sizes. However, this wasn't much of a big change and the accuracy of the model is still worse than the 0.775 which I achieved through a simple Random Decision Forest Model.**\n"]},{"cell_type":"markdown","metadata":{},"source":["**From this code, I realized that I should be imputing and cleaning the data BEFORE I use pd.get_dummies(). This is because pd.get_dummies() adds a lot more columns to the data because it turns all categorial columns to multiple binary columns which puts a 0 or 1 in each row. So, I should be cleaming up the data BEFORE I use pd.get_dummies() since pd.get_dummies adds so many more columns to the data and makes the data so much harder to clean. \n","For example, it's hard to impute values in the \"age\" column after you do pd.get_Dummies() since imputing age relies on finding the average age of each person and it's a lot easier to find the average age is the all the ages were in a single column but if we do pd.get_Dummies, we are going to split up the age column into many more age columns which correspond to each unique age value in the data set**"]},{"cell_type":"markdown","metadata":{},"source":["# Now I will re-initialize my data set and clean up my data before using pd.get_dummies()"]},{"cell_type":"markdown","metadata":{},"source":["**I set up X_train and Y_train, but this time, instead of doing pd.get_Dummies() on X_train, I'm not going to use pd.get_Dummies() until after I clean up the data**"]},{"cell_type":"code","execution_count":238,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.570822Z","iopub.status.busy":"2023-12-08T16:54:37.570221Z","iopub.status.idle":"2023-12-08T16:54:37.589769Z","shell.execute_reply":"2023-12-08T16:54:37.588219Z","shell.execute_reply.started":"2023-12-08T16:54:37.570788Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  \n"]}],"source":["train_data = pd.read_csv(\"train.csv\")\n","\n","features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"] # Here, I also added more features which I think might be useful\n","\n","X_train = train_data # I won't actually limit the X_train to just the columns of the features I want just yet. This is because I will use the other columns in the data set to help impute missing values\n","\n","Y_train = train_data[\"Survived\"]\n","\n","print(X_train.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["# Start cleaning up the data"]},{"cell_type":"code","execution_count":239,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.592301Z","iopub.status.busy":"2023-12-08T16:54:37.591487Z","iopub.status.idle":"2023-12-08T16:54:37.604267Z","shell.execute_reply":"2023-12-08T16:54:37.602156Z","shell.execute_reply.started":"2023-12-08T16:54:37.592251Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","177\n"]}],"source":["# Impute missing values for Pclass\n","\n","average_Pclass = X_train[\"Pclass\"].median()\n","\n","X_train[\"Pclass\"].fillna(average_Pclass, inplace = True)\n","\n","print(X_train[\"Pclass\"].isnull().sum()) # print the number of rows in the \"Pclass\" columns that now have no value\n","\n","print(X_train[\"Sex\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Sex\" column so we don't need to impute anything\n","\n","print(X_train[\"Fare\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Fare\" column so we don't need to impute anything\n","\n","print(X_train[\"SibSp\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"SibSp\" column so we don't need to impute anything\n","\n","print(X_train[\"Parch\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Parch\" column so we don't need to impute anything\n","\n","print(X_train[\"Age\"].isnull().sum()) # This prints out 177 so there are 177 rows with missing values in the \"Age\" column! This means we have to do a lot of imputing\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**From this, we learned that age is the main column with missing values (of the columns that we are working with) so we need to methodically impute those missing values**"]},{"cell_type":"markdown","metadata":{},"source":["# Cleaning up (imputing) the Age column using a Random Forest Regressor model (a type of Machine Learning model) that accurately predicts the age of a person given their "]},{"cell_type":"markdown","metadata":{},"source":["I made it so that when ever you run this code, it will check to see if there is already imputed data (inside of the csv) and if there is, then the imputer won't run since it takes a long time and if the imputed already exists (from previous runs), then there's no point in running the imputer again (which is what I did before and it wasted so much time)"]},{"cell_type":"code","execution_count":240,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["I used the data from the csv\n","0\n"]}],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import LabelEncoder\n","\n","imputed_csv_path = 'X_train_imputed.csv'\n","\n","if not os.path.exists(imputed_csv_path):\n","    # If CSV doesn't exist, run the imputation and save the results\n","   #We use \"LabelEncoder\" to turn all the categorial values (values which don't use numbers) into values which the Random Forest Regressor can understand more easily. This is very similar to doing something like pd.get_dummies()\n","\n","    label_encoders = {}\n","    for column in ['Sex', 'Embarked']:\n","        label_encoders[column] = LabelEncoder()\n","        # We use 'astype(str)' to convert any NaN values to a string representation\n","        X_train[column] = label_encoders[column].fit_transform(X_train[column].astype(str))\n","\n","    features = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","    # One very important thing that I will end up doing is using the \"Survived\" column of the dataset to predict the age. This is because there is a strong correleation between the survival and age. \n","\n","\n","    #This is an imputer. It is a function that can efficiently impute values in the way you want. For this scenario, I am using IterativeImputer() and the RandomForestRegressor(() to )\n","    imputer = IterativeImputer(RandomForestRegressor(n_estimators=10), max_iter=100, random_state=42)\n","\n","\n","    # Fit the imputer on the DataFrame with the features\n","    imputer.fit(X_train[features]) #When IterativeImputer is used alongside an ML model like RandomForestRegressor, calling .fit() on the IterativeImputer() object will also fit (train) the ML model\n","\n","\n","    # Perform the imputation on the Training Data. THIS WILL ACTUAL IMPUTE ANY MISSING VALUES IN EACH ROW, not only the missing values in Age.\n","    X_train_imputed = imputer.transform(X_train[features])\n","\n","\n","    # Convert the output back to a DataFrame\n","    X_train_imputed = pd.DataFrame(X_train_imputed, columns=features)\n","\n","    # Update the original DataFrame with the imputed values. This means that the columns of the original X_train dataframe that weren't part of the features (like \"Name\" isn't apart of the features list)\n","    X_train[features] = X_train_imputed\n","\n","    #print(X_train.head())\n","\n","\n","    # Save the imputed DataFrame to CSV\n","    X_train.to_csv(imputed_csv_path, index=False)\n","    print(\"I did imputation again\")\n","\n","else:\n","    # Load the imputed data from the CSV file\n","    X_train = pd.read_csv(imputed_csv_path)\n","    print(\"I used the data from the csv\")\n","\n","\n","print(X_train[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful "]},{"cell_type":"code","execution_count":241,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.606013Z","iopub.status.busy":"2023-12-08T16:54:37.605540Z","iopub.status.idle":"2023-12-08T16:54:40.964691Z","shell.execute_reply":"2023-12-08T16:54:40.959909Z","shell.execute_reply.started":"2023-12-08T16:54:37.605969Z"},"trusted":true},"outputs":[],"source":["# from sklearn.experimental import enable_iterative_imputer\n","# from sklearn.impute import IterativeImputer\n","# from sklearn.ensemble import RandomForestRegressor\n","# from sklearn.preprocessing import LabelEncoder\n","\n","\n","# #We use \"LabelEncoder\" to turn all the categorial values (values which don't use numbers) into values which the Random Forest Regressor can understand more easily. This is very similar to doing something like pd.get_dummies()\n","\n","# label_encoders = {}\n","# for column in ['Sex', 'Embarked']:\n","#     label_encoders[column] = LabelEncoder()\n","#     # We use 'astype(str)' to convert any NaN values to a string representation\n","#     X_train[column] = label_encoders[column].fit_transform(X_train[column].astype(str))\n","\n","# features = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","# # One very important thing that I will end up doing is using the \"Survived\" column of the dataset to predict the age. This is because there is a strong correleation between the survival and age. \n","\n","\n","# #This is an imputer. It is a function that can efficiently impute values in the way you want. For this scenario, I am using IterativeImputer() and the RandomForestRegressor(() to )\n","# imputer = IterativeImputer(RandomForestRegressor(n_estimators=10), max_iter=100, random_state=42)\n","\n","\n","# # Fit the imputer on the DataFrame with the features\n","# imputer.fit(X_train[features]) #When IterativeImputer is used alongside an ML model like RandomForestRegressor, calling .fit() on the IterativeImputer() object will also fit (train) the ML model\n","\n","\n","# # Perform the imputation on the Training Data. THIS WILL ACTUAL IMPUTE ANY MISSING VALUES IN EACH ROW, not only the missing values in Age.\n","# X_train_imputed = imputer.transform(X_train[features])\n","\n","\n","# # Convert the output back to a DataFrame\n","# X_train_imputed = pd.DataFrame(X_train_imputed, columns=features)\n","\n","# # Update the original DataFrame with the imputed values. This means that the columns of the original X_train dataframe that weren't part of the features (like \"Name\" isn't apart of the features list)\n","# X_train[features] = X_train_imputed\n","\n","# #print(X_train.head())\n","\n","# print(X_train[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful "]},{"cell_type":"markdown","metadata":{},"source":["**Using IterativeRegressor() and RandomForestRegressor(), I essentially used an ML model to impute any missing values in the dataset. I wanted to clean up the age column using the most effective method and I didn't want to just find the average age and impute all those values to the missing values. I wanted to be smart since age is an incredibly important feature.**"]},{"cell_type":"markdown","metadata":{},"source":["**Using the ImperativeImputer(), this will actually impute all the missing values in each row. It won't only impute the missing values in the \"age\" column. BUT, since we already cleaned the data and know there are 0 missing values for all the other columns that we care about (the ones with the features we are using), the imputer will be imputing values only to the missing values of age and other columns that we don't care about, but out of all the columns we care about, it will only be affecting the age column since all the other columns we care about have 0 missing values as we checked in the code previously.**"]},{"cell_type":"code","execution_count":242,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.966132Z","iopub.status.idle":"2023-12-08T16:54:40.966690Z","shell.execute_reply":"2023-12-08T16:54:40.966400Z","shell.execute_reply.started":"2023-12-08T16:54:40.966377Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   Pclass  Sex     Fare   Age  SibSp  Parch\n","0     3.0  1.0   7.2500  22.0    1.0    0.0\n","1     1.0  0.0  71.2833  38.0    1.0    0.0\n","2     3.0  0.0   7.9250  26.0    0.0    0.0\n","3     1.0  0.0  53.1000  35.0    1.0    0.0\n","4     3.0  1.0   8.0500  35.0    0.0    0.0\n","314\n","577\n","   Pclass     Sex     Fare   Age  SibSp  Parch\n","0     3.0    male   7.2500  22.0    1.0    0.0\n","1     1.0  female  71.2833  38.0    1.0    0.0\n","2     3.0  female   7.9250  26.0    0.0    0.0\n","3     1.0  female  53.1000  35.0    1.0    0.0\n","4     3.0    male   8.0500  35.0    0.0    0.0\n","577\n"]}],"source":["features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n","\n","X_train = X_train[features]\n","\n","print(X_train.head(5))\n","\n","print((X_train[\"Sex\"] == 0).sum())\n","print((X_train[\"Sex\"] == 1).sum())\n","\n","# Since the dataset is 65% male, we can see from printing out the number of people with \"sex equal to 0\" and \"sex equal to 1\" that the LabelEncoder made Male into \"1\" and Female into \"0\". Now, let's change it back to \"Male\" and \"Female\".\n","\n","X_train['Sex'] = X_train['Sex'].map({0: 'female', 1: 'male'})\n","\n","print(X_train.head(5))\n","\n","print((X_train[\"Sex\"] == \"male\").sum())"]},{"cell_type":"markdown","metadata":{},"source":["**Here, in the code above, I changed X_train back to focus only on the features we care about. I also changed the \"Sex\" column from 0 and 1 back to Male and Female.**"]},{"cell_type":"markdown","metadata":{},"source":["# After the careful inspection of the test.csv, I see that there are also missing values in the test.csv. As a result, we have to clean up the data for test.csv as we did for train.csv"]},{"cell_type":"markdown","metadata":{},"source":["**Seeing which columns have missing values:**"]},{"cell_type":"code","execution_count":243,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.968796Z","iopub.status.idle":"2023-12-08T16:54:40.969752Z","shell.execute_reply":"2023-12-08T16:54:40.969564Z","shell.execute_reply.started":"2023-12-08T16:54:40.969535Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","0\n","0\n","0\n","1\n","86\n"]}],"source":["test_data = pd.read_csv(\"test.csv\")\n","\n","features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n","\n","print(test_data[\"Pclass\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"Sex\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"SibSp\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"Parch\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"Fare\"].isnull().sum()) # Since this is 1, that means there is one missing value\n","\n","print(test_data[\"Age\"].isnull().sum()) # Since this is 86, that means there are 86 missing values\n"]},{"cell_type":"markdown","metadata":{},"source":["**From this, we can see that only \"Fare\" annd \"Age\" have missing values. Since \"Fare\" only has one missing value, it's not worth it to go to through all the work to do some advanced imputation technique to fill in missing values. So, I will just fill that in with the average fare value.**\n","\n","**For the \"Age\" feature, I will repeat the process of using an IterativeImputer and a RandomForestRegressor to imputate missing values**"]},{"cell_type":"code","execution_count":244,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.970708Z","iopub.status.idle":"2023-12-08T16:54:40.971089Z","shell.execute_reply":"2023-12-08T16:54:40.970926Z","shell.execute_reply.started":"2023-12-08T16:54:40.970909Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","86\n"]}],"source":["average_fare = test_data[\"Fare\"].mean()\n","\n","test_data[\"Fare\"].fillna(average_fare, inplace = True) # This replaces the single missing value in \"Fare\" with the average value Fare Value of the test data\n","\n","print(test_data[\"Fare\"].isnull().sum())\n","\n","print(test_data[\"Age\"].isnull().sum())"]},{"cell_type":"markdown","metadata":{},"source":["I made it so that when ever you run this code, it will check to see if there is already imputed data (inside of the csv) and if there is, then the imputer won't run since it takes a long time and if the imputed already exists (from previous runs), then there's no point in running the imputer again (which is what I did before and it wasted so much time)"]},{"cell_type":"code","execution_count":245,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["I used imputated data from previous csv\n","0\n"]}],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Your imputation code setup here...\n","\n","# Path to the imputed CSV file\n","imputed_csv_path = 'test_data_imputed.csv'\n","\n","# Check if the imputed data CSV file exists\n","if not os.path.exists(imputed_csv_path):\n","    # If CSV doesn't exist, run the imputation and save the results\n","    #We use \"LabelEncoder\" to turn all the categorial values (values which don't use numbers) into values which the Random Forest Regressor can understand more easily. This is very similar to doing something like pd.get_dummies()\n","\n","    label_encoders = {}\n","    for column in ['Sex', 'Embarked']:\n","        label_encoders[column] = LabelEncoder()\n","        # We use 'astype(str)' to convert any NaN values to a string representation\n","        test_data[column] = label_encoders[column].fit_transform(test_data[column].astype(str))\n","\n","    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","\n","\n","    #This is an imputer. It is a function that can efficiently impute values in the way you want. For this scenario, I am using IterativeImputer() and the RandomForestRegressor(() to )\n","    imputer = IterativeImputer(RandomForestRegressor(n_estimators=10), max_iter=100, random_state=42)\n","\n","\n","    # Fit the imputer on the DataFrame with the features\n","    imputer.fit(test_data[features]) #When IterativeImputer is used alongside an ML model like RandomForestRegressor, calling .fit() on the IterativeImputer() object will also fit (train) the ML model\n","\n","\n","    # Perform the imputation on the Test Data. THIS WILL ACTUAL IMPUTE ANY MISSING VALUES IN EACH ROW, not only the missing values in Age.\n","    test_data_imputed = imputer.transform(test_data[features])\n","\n","\n","    # Convert the output back to a DataFrame\n","    test_data_imputed = pd.DataFrame(test_data_imputed, columns=features)\n","\n","    # Update the original DataFrame with the imputed values. This means that the columns of the original test_data dataframe that weren't part of the features (like \"Name\" isn't apart of the features list)\n","    test_data[features] = test_data_imputed\n","\n","    print(test_data[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful \n","\n","    print(test_data.head())\n","\n","    # Save the imputed DataFrame to CSV\n","    test_data.to_csv(imputed_csv_path, index=False)\n","\n","    print(\"I created a new CSV file with imputation\")\n","\n","\n","else:\n","    # Load the imputed data from the CSV file\n","    test_data = pd.read_csv(imputed_csv_path)\n","\n","    print(\"I used imputated data from previous csv\")\n","\n","print(test_data[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful \n"]},{"cell_type":"code","execution_count":246,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.973377Z","iopub.status.idle":"2023-12-08T16:54:40.974299Z","shell.execute_reply":"2023-12-08T16:54:40.974081Z","shell.execute_reply.started":"2023-12-08T16:54:40.974055Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","   PassengerId  Pclass                                          Name  Sex  \\\n","0          892     3.0                              Kelly, Mr. James  1.0   \n","1          893     3.0              Wilkes, Mrs. James (Ellen Needs)  0.0   \n","2          894     2.0                     Myles, Mr. Thomas Francis  1.0   \n","3          895     3.0                              Wirz, Mr. Albert  1.0   \n","4          896     3.0  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  0.0   \n","\n","    Age  SibSp  Parch   Ticket     Fare Cabin  Embarked  \n","0  34.5    0.0    0.0   330911   7.8292   NaN       1.0  \n","1  47.0    1.0    0.0   363272   7.0000   NaN       2.0  \n","2  62.0    0.0    0.0   240276   9.6875   NaN       1.0  \n","3  27.0    0.0    0.0   315154   8.6625   NaN       2.0  \n","4  22.0    1.0    1.0  3101298  12.2875   NaN       2.0  \n"]}],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","#We use \"LabelEncoder\" to turn all the categorial values (values which don't use numbers) into values which the Random Forest Regressor can understand more easily. This is very similar to doing something like pd.get_dummies()\n","\n","label_encoders = {}\n","for column in ['Sex', 'Embarked']:\n","    label_encoders[column] = LabelEncoder()\n","    # We use 'astype(str)' to convert any NaN values to a string representation\n","    test_data[column] = label_encoders[column].fit_transform(test_data[column].astype(str))\n","\n","features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","\n","\n","#This is an imputer. It is a function that can efficiently impute values in the way you want. For this scenario, I am using IterativeImputer() and the RandomForestRegressor(() to )\n","imputer = IterativeImputer(RandomForestRegressor(n_estimators=10), max_iter=100, random_state=42)\n","\n","\n","# Fit the imputer on the DataFrame with the features\n","imputer.fit(test_data[features]) #When IterativeImputer is used alongside an ML model like RandomForestRegressor, calling .fit() on the IterativeImputer() object will also fit (train) the ML model\n","\n","\n","# Perform the imputation on the Test Data. THIS WILL ACTUAL IMPUTE ANY MISSING VALUES IN EACH ROW, not only the missing values in Age.\n","test_data_imputed = imputer.transform(test_data[features])\n","\n","\n","# Convert the output back to a DataFrame\n","test_data_imputed = pd.DataFrame(test_data_imputed, columns=features)\n","\n","# Update the original DataFrame with the imputed values. This means that the columns of the original test_data dataframe that weren't part of the features (like \"Name\" isn't apart of the features list)\n","test_data[features] = test_data_imputed\n","\n","print(test_data[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful \n","\n","print(test_data.head())"]},{"cell_type":"markdown","metadata":{},"source":["**The code above will imputate all the missing values in the test_data**"]},{"cell_type":"markdown","metadata":{},"source":["# Finalize the cleaning of the data and make it so test_data only has the features we want"]},{"cell_type":"code","execution_count":247,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.976046Z","iopub.status.idle":"2023-12-08T16:54:40.977148Z","shell.execute_reply":"2023-12-08T16:54:40.976862Z","shell.execute_reply.started":"2023-12-08T16:54:40.976837Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["152\n","266\n","   Pclass     Sex     Fare   Age  SibSp  Parch\n","0     3.0    male   7.8292  34.5    0.0    0.0\n","1     3.0  female   7.0000  47.0    1.0    0.0\n","2     2.0    male   9.6875  62.0    0.0    0.0\n","3     3.0    male   8.6625  27.0    0.0    0.0\n","4     3.0  female  12.2875  22.0    1.0    1.0\n","266\n"]}],"source":["features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n","\n","test_data = test_data[features]\n","\n","print((test_data[\"Sex\"] == 0).sum())\n","print((test_data[\"Sex\"] == 1).sum())\n","\n","# Since the dataset is 65% male, we can see from printing out the number of people with \"sex equal to 0\" and \"sex equal to 1\" that the LabelEncoder made Male into \"1\" and Female into \"0\". Now, let's change it back to \"male\" and \"female\".\n","\n","test_data['Sex'] = test_data['Sex'].map({0: 'female', 1: 'male'})\n","\n","print(test_data.head(5))\n","\n","print((test_data[\"Sex\"] == \"male\").sum())"]},{"cell_type":"markdown","metadata":{},"source":["# Now, after cleaning up the data, I can split up the data into train_validation_test "]},{"cell_type":"code","execution_count":248,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.978124Z","iopub.status.idle":"2023-12-08T16:54:40.978609Z","shell.execute_reply":"2023-12-08T16:54:40.978469Z","shell.execute_reply.started":"2023-12-08T16:54:40.978453Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","Y_train = train_data[\"Survived\"]\n","\n","# First, split into training (60%) and a test set (40%)\n","train_X, test_X, train_Y, test_Y = train_test_split(X_train, Y_train, test_size=0.4, random_state = 42)\n","\n","#Second, split test set into a validation (20%) and test set (20%)\n","valid_X, test_X, valid_Y, test_Y= train_test_split(test_X, test_Y, test_size = 0.5, random_state = 42)"]},{"cell_type":"markdown","metadata":{},"source":["# Using pd.get_dummies() on all the data"]},{"cell_type":"markdown","metadata":{},"source":["**Setting up the Test data (from Test.csv) (This is NOT the test data from the Train_validation_test split):**"]},{"cell_type":"code","execution_count":249,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.980483Z","iopub.status.idle":"2023-12-08T16:54:40.981008Z","shell.execute_reply":"2023-12-08T16:54:40.980791Z","shell.execute_reply.started":"2023-12-08T16:54:40.980767Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   Pclass     Fare   Age  SibSp  Parch  Sex_female  Sex_male\n","0     3.0   7.8292  34.5    0.0    0.0       False      True\n","1     3.0   7.0000  47.0    1.0    0.0        True     False\n","2     2.0   9.6875  62.0    0.0    0.0       False      True\n","3     3.0   8.6625  27.0    0.0    0.0       False      True\n","4     3.0  12.2875  22.0    1.0    1.0        True     False\n"]}],"source":["test_data = test_data #the \"test_data\" variable here is after we cleaned up the test_data \n","\n","features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n","\n","test_data = test_data[features]\n","\n","test_data = pd.get_dummies(test_data)\n","print(test_data.head())"]},{"cell_type":"markdown","metadata":{},"source":["**Setting up the Train data and Validation data and the Test data from the train_validation_test split:**"]},{"cell_type":"code","execution_count":250,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.982767Z","iopub.status.idle":"2023-12-08T16:54:40.983157Z","shell.execute_reply":"2023-12-08T16:54:40.983015Z","shell.execute_reply.started":"2023-12-08T16:54:40.982993Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     Pclass     Fare   Age  SibSp  Parch  Sex_female  Sex_male\n","570     2.0  10.5000  62.0    0.0    0.0       False      True\n","787     3.0  29.1250   8.0    4.0    1.0       False      True\n","74      3.0  56.4958  32.0    0.0    0.0       False      True\n","113     3.0   9.8250  20.0    1.0    0.0        True     False\n","635     2.0  13.0000  28.0    0.0    0.0        True     False\n","     Pclass     Fare   Age  SibSp  Parch  Sex_female  Sex_male\n","849     1.0  89.1042  37.1    1.0    0.0        True     False\n","331     1.0  28.5000  45.5    0.0    0.0       False      True\n","260     3.0   7.7500  46.6    0.0    0.0       False      True\n","316     2.0  26.0000  24.0    1.0    0.0        True     False\n","292     2.0  12.8750  36.0    0.0    0.0       False      True\n","     Pclass     Fare        Age  SibSp  Parch  Sex_female  Sex_male\n","424     3.0  20.2125  18.000000    1.0    1.0       False      True\n","837     3.0   8.0500  31.438093    0.0    0.0       False      True\n","525     3.0   7.7500  40.500000    0.0    0.0       False      True\n","328     3.0  20.5250  31.000000    1.0    1.0        True     False\n","70      2.0  10.5000  32.000000    0.0    0.0       False      True\n"]}],"source":["train_X = pd.get_dummies(train_X)\n","print(train_X.head())\n","\n","valid_X = pd.get_dummies(valid_X)\n","print(valid_X.head())\n","\n","test_X = pd.get_dummies(test_X)\n","print(test_X.head())"]},{"cell_type":"markdown","metadata":{},"source":["# Making the model, training it using train_X and changing the model based on valid_X"]},{"cell_type":"code","execution_count":251,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.984167Z","iopub.status.idle":"2023-12-08T16:54:40.984497Z","shell.execute_reply":"2023-12-08T16:54:40.984354Z","shell.execute_reply.started":"2023-12-08T16:54:40.984339Z"},"trusted":true},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Commented out the code so that the model won't run everytime\n","\n","# model = MLPClassifier(hidden_layer_sizes = (20, 11), activation = \"relu\", solver = \"lbfgs\", max_iter = 1000, random_state = 42)\n","\n","# model.fit(train_X, train_Y)\n","\n","# y_pred = model.predict(valid_X)\n","\n","# print(f\"Accuracy: {accuracy_score(valid_Y, y_pred)}\")"]},{"cell_type":"markdown","metadata":{},"source":["**From the code above, you can see that I am using the validation set to test out the code**"]},{"cell_type":"code","execution_count":252,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.985885Z","iopub.status.idle":"2023-12-08T16:54:40.986231Z","shell.execute_reply":"2023-12-08T16:54:40.986106Z","shell.execute_reply.started":"2023-12-08T16:54:40.986091Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","# y_pred = model.predict(test_X)\n","\n","# print(f\"Accuracy: {accuracy_score(test_Y, y_pred)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Using GridSearchCV to find the best parameters for my MLPClassifier Model"]},{"cell_type":"markdown","metadata":{},"source":["**The \"parameter space\" are all the different components and possible values which I want to try for my model**"]},{"cell_type":"code","execution_count":253,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.987483Z","iopub.status.idle":"2023-12-08T16:54:40.987795Z","shell.execute_reply":"2023-12-08T16:54:40.987666Z","shell.execute_reply.started":"2023-12-08T16:54:40.987651Z"},"trusted":true},"outputs":[],"source":["# %%capture --no-stderr\n","\n","# # Code that causes warnings\n","# import warnings\n","# from sklearn.exceptions import ConvergenceWarning\n","\n","# # Suppress ConvergenceWarning\n","# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","# warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.neural_network import MLPClassifier\n","\n","training_X = pd.concat([train_X, valid_X, test_X], ignore_index=True)\n","training_Y = pd.concat([train_Y, valid_Y, test_Y], ignore_index=True)\n","\n","possible_hidden_layers = []\n","\n","for i in range(8,16):\n","    for j in range(8,16):\n","        possible_hidden_layers.append((i, j))\n","\n","parameter_space = {\n","    'hidden_layer_sizes': possible_hidden_layers,\n","    'activation': ['tanh', 'relu'],\n","    'solver': ['sgd', 'adam', \"lbfgs\"],\n","    'alpha': [0.0001, 0.05],\n","    'learning_rate': ['constant','adaptive'],\n","}\n","\n","# mlp = MLPClassifier(max_iter=1000, random_state = 42)\n","\n","# clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n","\n","# clf.fit(training_X, training_Y)\n","\n","# print('Best parameters found:\\n', clf.best_params_)\n","\n","# print('Best score (CV mean):', clf.best_score_)\n","\n","# print(\"Done\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Now that I found the best hyperparameters, I can finalize my model"]},{"cell_type":"markdown","metadata":{},"source":["**Test using validation set:**"]},{"cell_type":"code","execution_count":254,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.989534Z","iopub.status.idle":"2023-12-08T16:54:40.990230Z","shell.execute_reply":"2023-12-08T16:54:40.989882Z","shell.execute_reply.started":"2023-12-08T16:54:40.989853Z"},"trusted":true},"outputs":[],"source":["# Best parameters: {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (13, 15), 'learning_rate': 'constant', 'solver': 'lbfgs'}\n","# Best score (CV mean): 0.8282828282828283\n","\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Commented out the code so that the model won't run everytime\n","\n","# model = MLPClassifier(activation = \"tanh\", alpha = 0.05, hidden_layer_sizes = (13, 15), learning_rate = \"constant\", solver = \"lbfgs\", max_iter=5000, random_state = 42)\n","\n","# #model = MLPClassifier(activation = \"relu\", hidden_layer_sizes = (20, 11), solver = \"lbfgs\",  max_iter=2000, random_state = 42)\n","\n","# model.fit(train_X, train_Y)\n","\n","# y_pred = model.predict(valid_X)\n","\n","# print(f\"Accuracy: {accuracy_score(valid_Y, y_pred)}\")"]},{"cell_type":"markdown","metadata":{},"source":["**Test using test set (from Train-validation-test split)**"]},{"cell_type":"code","execution_count":255,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.995927Z","iopub.status.idle":"2023-12-08T16:54:40.997205Z","shell.execute_reply":"2023-12-08T16:54:40.997015Z","shell.execute_reply.started":"2023-12-08T16:54:40.996989Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","# y_pred = model.predict(test_X)\n","\n","# print(f\"Accuracy: {accuracy_score(test_Y, y_pred)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Submitting Model 3"]},{"cell_type":"code","execution_count":256,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.998549Z","iopub.status.idle":"2023-12-08T16:54:40.999093Z","shell.execute_reply":"2023-12-08T16:54:40.998833Z","shell.execute_reply.started":"2023-12-08T16:54:40.998811Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","# full_X = pd.get_dummies(X_train)\n","\n","# full_Y = Y_train\n","\n","# #We end up using the WHOLE training dataset (all of train.csv) to fit (train) our model right before submission since this gives our model the most amount of data to train on\n","# model.fit(full_X, full_Y)\n","\n","\n","# predictions3 = model.predict(test_data)"]},{"cell_type":"code","execution_count":257,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.000528Z","iopub.status.idle":"2023-12-08T16:54:41.000927Z","shell.execute_reply":"2023-12-08T16:54:41.000769Z","shell.execute_reply.started":"2023-12-08T16:54:41.000751Z"},"trusted":true},"outputs":[],"source":["original_test = pd.read_csv(\"test.csv\")\n","\n","pass_id = original_test['PassengerId']\n","\n","# original_test = original_test.drop('PassengerId', axis=1)\n","# original_test = pd.get_dummies(original_test[features])\n","\n","test_data[\"PassengerId\"] = pass_id\n","\n","# Commented out the code so that the model won't run everytime\n","\n","# predictions3 = predictions3.astype(int)\n","\n","#print(predictions3)\n","\n","# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions3})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data = test_data.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["# Changing the model solver from \"lbfgs\" to \"adam\""]},{"cell_type":"code","execution_count":258,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.001879Z","iopub.status.idle":"2023-12-08T16:54:41.002239Z","shell.execute_reply":"2023-12-08T16:54:41.002097Z","shell.execute_reply.started":"2023-12-08T16:54:41.002082Z"},"trusted":true},"outputs":[],"source":["model = MLPClassifier(activation = \"tanh\", alpha = 0.05, hidden_layer_sizes = (13, 15), learning_rate = \"constant\", solver = \"adam\", max_iter=5000, random_state = 42)\n","# Try changing it back to lbfgs after normalization since maybe it converges faster now after normalization\n","#Also, with normalization, you can increase the learning_rate with less risk of divergence which is good\n"]},{"cell_type":"markdown","metadata":{},"source":["**I made this change because it said that lbfgs was not converging so I tried changing to a differnet solver and adam seemed to converge. But also, I am going to normalize my data which should make the training of my data converge faster. So, I might also go back to lbfgs to see if it converges now**"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Engineering: Putting more weight onto specific features (like Gender and Age)"]},{"cell_type":"markdown","metadata":{},"source":["**We know from history that Women and Children were more likely to survive so it makes more sense to put more weight into those features. In other words, we should penalize a person more if they are not a Women or Child and therefore make it less likely for them to survive**"]},{"cell_type":"code","execution_count":259,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.002983Z","iopub.status.idle":"2023-12-08T16:54:41.003288Z","shell.execute_reply":"2023-12-08T16:54:41.003161Z","shell.execute_reply.started":"2023-12-08T16:54:41.003147Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     Pclass      Fare   Age  SibSp  Parch  Sex_female  Sex_male\n","570     2.0   10.5000  62.0    0.0    0.0       False      True\n","787     3.0   29.1250   8.0    4.0    1.0       False      True\n","74      3.0   56.4958  32.0    0.0    0.0       False      True\n","113     3.0    9.8250  20.0    1.0    0.0        True     False\n","635     2.0   13.0000  28.0    0.0    0.0        True     False\n","..      ...       ...   ...    ...    ...         ...       ...\n","106     3.0    7.6500  21.0    0.0    0.0        True     False\n","270     1.0   31.0000  44.8    0.0    0.0       False      True\n","860     3.0   14.1083  41.0    2.0    0.0       False      True\n","435     1.0  120.0000  14.0    1.0    2.0        True     False\n","102     1.0   77.2875  21.0    0.0    1.0       False      True\n","\n","[534 rows x 7 columns]\n","     Pclass      Fare   Age  SibSp  Parch  Sex_female  Sex_male  \\\n","570     2.0   10.5000  62.0    0.0    0.0       False      True   \n","787     3.0   29.1250   8.0    4.0    1.0       False      True   \n","74      3.0   56.4958  32.0    0.0    0.0       False      True   \n","113     3.0    9.8250  20.0    1.0    0.0        True     False   \n","635     2.0   13.0000  28.0    0.0    0.0        True     False   \n","..      ...       ...   ...    ...    ...         ...       ...   \n","106     3.0    7.6500  21.0    0.0    0.0        True     False   \n","270     1.0   31.0000  44.8    0.0    0.0       False      True   \n","860     3.0   14.1083  41.0    2.0    0.0       False      True   \n","435     1.0  120.0000  14.0    1.0    2.0        True     False   \n","102     1.0   77.2875  21.0    0.0    1.0       False      True   \n","\n","     FemaleFirstClass  ChildFirstClass  IsChild  FamilySize  IsAlone  \n","570                 0                0        0         1.0        1  \n","787                 0                0        1         6.0        0  \n","74                  0                0        0         1.0        1  \n","113                 0                0        0         2.0        0  \n","635                 0                0        0         1.0        1  \n","..                ...              ...      ...         ...      ...  \n","106                 0                0        0         1.0        1  \n","270                 0                0        0         1.0        1  \n","860                 0                0        0         3.0        0  \n","435                 1                1        1         4.0        0  \n","102                 0                0        0         2.0        0  \n","\n","[534 rows x 12 columns]\n"]}],"source":["print(train_X)\n","\n","train_X['FemaleFirstClass'] = ((train_X['Sex_female'] == True) & (train_X['Pclass'] == 1)).astype(int)\n","train_X['ChildFirstClass'] = ((train_X['Age'] < 18) & (train_X['Pclass'] == 1)).astype(int) \n","\n","\n","valid_X['FemaleFirstClass'] = ((valid_X['Sex_female'] == True) & (valid_X['Pclass'] == 1)).astype(int)\n","valid_X['ChildFirstClass'] = ((valid_X['Age'] < 18) & (valid_X['Pclass'] == 1)).astype(int) \n","\n","\n","test_X['FemaleFirstClass'] = ((test_X['Sex_female'] == True) & (test_X['Pclass'] == 1)).astype(int)\n","test_X['ChildFirstClass'] = ((test_X['Age'] < 18) & (test_X['Pclass'] == 1)).astype(int) \n","\n","\n","test_data['FemaleFirstClass'] = ((test_data['Sex_female'] == True) & (test_data['Pclass'] == 1)).astype(int)\n","test_data['ChildFirstClass'] = ((test_data['Age'] < 18) & (test_data['Pclass'] == 1)).astype(int) \n","\n","\n","# Bin 'Age' into categories\n","train_X['IsChild'] = (train_X['Age'] < 18).astype(int)\n","valid_X['IsChild'] = (valid_X['Age'] < 18).astype(int)\n","test_X['IsChild'] = (test_X['Age'] < 18).astype(int)\n","test_data['IsChild'] = (test_data['Age'] < 18).astype(int)\n","\n","\n","# Create FamilySize and IsAlone\n","train_X['FamilySize'] = train_X['SibSp'] + train_X['Parch'] + 1\n","valid_X['FamilySize'] = valid_X['SibSp'] + valid_X['Parch'] + 1\n","test_X['FamilySize'] = test_X['SibSp'] + test_X['Parch'] + 1\n","test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n","\n","\n","train_X['IsAlone'] = (train_X['FamilySize'] == 1).astype(int)\n","valid_X['IsAlone'] = (valid_X['FamilySize'] == 1).astype(int)\n","test_X['IsAlone'] = (test_X['FamilySize'] == 1).astype(int)\n","test_data['IsAlone'] = (test_data['FamilySize'] == 1).astype(int)\n","\n","\n","print(train_X)"]},{"cell_type":"markdown","metadata":{},"source":["# Normalizing all the data"]},{"cell_type":"markdown","metadata":{},"source":["**Creating the Normalizing Scaler**"]},{"cell_type":"code","execution_count":260,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.003933Z","iopub.status.idle":"2023-12-08T16:54:41.004243Z","shell.execute_reply":"2023-12-08T16:54:41.004123Z","shell.execute_reply.started":"2023-12-08T16:54:41.004109Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     Pclass      Fare   Age  SibSp  Parch  Sex_female  Sex_male  \\\n","570     2.0   10.5000  62.0    0.0    0.0       False      True   \n","787     3.0   29.1250   8.0    4.0    1.0       False      True   \n","74      3.0   56.4958  32.0    0.0    0.0       False      True   \n","113     3.0    9.8250  20.0    1.0    0.0        True     False   \n","635     2.0   13.0000  28.0    0.0    0.0        True     False   \n","..      ...       ...   ...    ...    ...         ...       ...   \n","106     3.0    7.6500  21.0    0.0    0.0        True     False   \n","270     1.0   31.0000  44.8    0.0    0.0       False      True   \n","860     3.0   14.1083  41.0    2.0    0.0       False      True   \n","435     1.0  120.0000  14.0    1.0    2.0        True     False   \n","102     1.0   77.2875  21.0    0.0    1.0       False      True   \n","\n","     FemaleFirstClass  ChildFirstClass  IsChild  FamilySize  IsAlone  \n","570                 0                0        0         1.0        1  \n","787                 0                0        1         6.0        0  \n","74                  0                0        0         1.0        1  \n","113                 0                0        0         2.0        0  \n","635                 0                0        0         1.0        1  \n","..                ...              ...      ...         ...      ...  \n","106                 0                0        0         1.0        1  \n","270                 0                0        0         1.0        1  \n","860                 0                0        0         3.0        0  \n","435                 1                1        1         4.0        0  \n","102                 0                0        0         2.0        0  \n","\n","[534 rows x 12 columns]\n"]}],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Select columns you want to normalize\n","columns_to_normalize = [\"Pclass\", \"Fare\",\"Age\", \"SibSp\"]\n","\n","scaler.fit(train_X[columns_to_normalize]) #it is important to fit the scaler on the training data (from the train-validation-test split) only\n","\n","print(train_X)"]},{"cell_type":"markdown","metadata":{},"source":["**It is very important to only fit the scaler on the training data from the train-validation-test split and not the training data from the whole train.csv since the whole training data includes data from the validation and test split and fitting on that will cause the testing (using validation and test) of the model to be overly optimistic. This is called data leakage.**\n","\n","**Also, it's important that we do not fit the scaler multiple times since that would cause inconsistant fitting throughout the data. It should only be fitted once and then applied to all the different datasets.**\n","\n","**However,before I submit Model 4, I should re-fit the normalization scaler on the entire training data since I will be running my model on the entire training data**"]},{"cell_type":"markdown","metadata":{},"source":["**Normalizing the test data (the one from test.csv)**"]},{"cell_type":"code","execution_count":261,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.006515Z","iopub.status.idle":"2023-12-08T16:54:41.006902Z","shell.execute_reply":"2023-12-08T16:54:41.006740Z","shell.execute_reply.started":"2023-12-08T16:54:41.006721Z"},"trusted":true},"outputs":[],"source":["test_data = test_data #the \"test_data\" variable here is after we cleaned up the test_data \n","\n","#print(test_data)\n","\n","# Fit the scaler on the data and then transform it\n","test_data[columns_to_normalize] = scaler.transform(test_data[columns_to_normalize])\n","\n","#print(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["**Normalizing the train, validation, and test data**"]},{"cell_type":"code","execution_count":262,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.007767Z","iopub.status.idle":"2023-12-08T16:54:41.008134Z","shell.execute_reply":"2023-12-08T16:54:41.007988Z","shell.execute_reply.started":"2023-12-08T16:54:41.007938Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     Pclass      Fare       Age  SibSp  Parch  Sex_female  Sex_male  \\\n","570     0.5  0.020495  0.773813  0.000    0.0       False      True   \n","787     1.0  0.056848  0.095250  0.500    1.0       False      True   \n","74      1.0  0.110272  0.396833  0.000    0.0       False      True   \n","113     1.0  0.019177  0.246042  0.125    0.0        True     False   \n","635     0.5  0.025374  0.346569  0.000    0.0        True     False   \n","\n","     FemaleFirstClass  ChildFirstClass  IsChild  FamilySize  IsAlone  \n","570                 0                0        0         1.0        1  \n","787                 0                0        1         6.0        0  \n","74                  0                0        0         1.0        1  \n","113                 0                0        0         2.0        0  \n","635                 0                0        0         1.0        1  \n","     Pclass      Fare       Age  SibSp  Parch  Sex_female  Sex_male  \\\n","849     0.0  0.173920  0.460920  0.125    0.0        True     False   \n","331     0.0  0.055628  0.566474  0.000    0.0       False      True   \n","260     1.0  0.015127  0.580297  0.000    0.0       False      True   \n","316     0.5  0.050749  0.296306  0.125    0.0        True     False   \n","292     0.5  0.025130  0.447097  0.000    0.0       False      True   \n","\n","     FemaleFirstClass  ChildFirstClass  IsChild  FamilySize  IsAlone  \n","849                 1                0        0         2.0        0  \n","331                 0                0        0         1.0        1  \n","260                 0                0        0         1.0        1  \n","316                 0                0        0         2.0        0  \n","292                 0                0        0         1.0        1  \n","     Pclass      Fare       Age  SibSp  Parch  Sex_female  Sex_male  \\\n","424     1.0  0.039452  0.220910  0.125    1.0       False      True   \n","837     1.0  0.015713  0.389772  0.000    0.0       False      True   \n","525     1.0  0.015127  0.503644  0.000    0.0       False      True   \n","328     1.0  0.040062  0.384267  0.125    1.0        True     False   \n","70      0.5  0.020495  0.396833  0.000    0.0       False      True   \n","\n","     FemaleFirstClass  ChildFirstClass  IsChild  FamilySize  IsAlone  \n","424                 0                0        0         3.0        0  \n","837                 0                0        0         1.0        1  \n","525                 0                0        0         1.0        1  \n","328                 0                0        0         3.0        0  \n","70                  0                0        0         1.0        1  \n"]}],"source":["train_X[columns_to_normalize] = scaler.transform(train_X[columns_to_normalize])\n","print(train_X.head())\n","\n","valid_X[columns_to_normalize] = scaler.transform(valid_X[columns_to_normalize])\n","print(valid_X.head())\n","\n","test_X[columns_to_normalize] = scaler.transform(test_X[columns_to_normalize])\n","print(test_X.head())"]},{"cell_type":"markdown","metadata":{},"source":["# Testing out my model (4th) after Feature Enginering and Normalization"]},{"cell_type":"code","execution_count":263,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.009850Z","iopub.status.idle":"2023-12-08T16:54:41.010200Z","shell.execute_reply":"2023-12-08T16:54:41.010069Z","shell.execute_reply.started":"2023-12-08T16:54:41.010053Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.8033707865168539\n"]}],"source":["model = MLPClassifier(activation = \"tanh\", alpha = 0, hidden_layer_sizes = (13, 15), learning_rate = \"constant\", solver = \"adam\", max_iter=5000, random_state = 42)\n","\n","model.fit(train_X, train_Y)\n","\n","y_pred = model.predict(valid_X)\n","\n","print(f\"Validation Accuracy: {accuracy_score(valid_Y, y_pred)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Using GridSearchCV to find new best amount of neurons per hidden layer and also to find best alpha and activation\n"]},{"cell_type":"code","execution_count":264,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.011033Z","iopub.status.idle":"2023-12-08T16:54:41.011325Z","shell.execute_reply":"2023-12-08T16:54:41.011202Z","shell.execute_reply.started":"2023-12-08T16:54:41.011188Z"},"trusted":true},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","\n","training_X = pd.concat([train_X, valid_X, test_X], ignore_index=True)\n","training_Y = pd.concat([train_Y, valid_Y, test_Y], ignore_index=True)\n","\n","possible_hidden_layers = []\n","\n","for i in range(8,20):\n","    for j in range(8,20):\n","        possible_hidden_layers.append((i, j))\n","\n","parameter_space = {\n","    'hidden_layer_sizes': possible_hidden_layers,\n","    'activation': ['tanh', 'relu'],\n","    'alpha': [0.0001, 0.05],\n","    \"max_iter\": [4000, 5000, 6000, 7000]\n","}\n","\n","# mlp = MLPClassifier(solver = \"adam\", random_state = 42)\n","\n","# clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n","\n","# clf.fit(training_X, training_Y)\n","\n","# print('Best parameters found:\\n', clf.best_params_)\n","\n","# print('Best score (CV mean):', clf.best_score_)\n","\n","# print(\"Done\")"]},{"cell_type":"markdown","metadata":{},"source":["# Before I submit Model 4, I should re-fit the normalization scaler on the entire training data since I will be running my model on the entire training data\n"]},{"cell_type":"code","execution_count":265,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.012335Z","iopub.status.idle":"2023-12-08T16:54:41.012615Z","shell.execute_reply":"2023-12-08T16:54:41.012495Z","shell.execute_reply.started":"2023-12-08T16:54:41.012481Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     Pclass      Fare       Age  SibSp  Parch  Sex_female  Sex_male  \\\n","0       0.5  0.020495  0.773813  0.000    0.0       False      True   \n","1       1.0  0.056848  0.095250  0.500    1.0       False      True   \n","2       1.0  0.110272  0.396833  0.000    0.0       False      True   \n","3       1.0  0.019177  0.246042  0.125    0.0        True     False   \n","4       0.5  0.025374  0.346569  0.000    0.0        True     False   \n","..      ...       ...       ...    ...    ...         ...       ...   \n","886     1.0  0.015713  0.258608  0.000    0.0       False      True   \n","887     0.0  0.060508  0.371701  0.000    0.0        True     False   \n","888     1.0  0.015713  0.547625  0.000    0.0       False      True   \n","889     0.0  0.512122  0.258608  0.250    2.0        True     False   \n","890     0.0  0.444099  0.472229  0.000    0.0        True     False   \n","\n","     FemaleFirstClass  ChildFirstClass  IsChild  FamilySize  IsAlone  \n","0                   0                0        0         1.0        1  \n","1                   0                0        1         6.0        0  \n","2                   0                0        0         1.0        1  \n","3                   0                0        0         2.0        0  \n","4                   0                0        0         1.0        1  \n","..                ...              ...      ...         ...      ...  \n","886                 0                0        0         1.0        1  \n","887                 1                0        0         1.0        1  \n","888                 0                0        0         1.0        1  \n","889                 1                0        0         5.0        0  \n","890                 1                0        0         1.0        1  \n","\n","[891 rows x 12 columns]\n","     Pclass      Fare       Age  SibSp  Parch  Sex_female  Sex_male  \\\n","0       0.5  0.020495  0.773813  0.000    0.0       False      True   \n","1       1.0  0.056848  0.095250  0.500    1.0       False      True   \n","2       1.0  0.110272  0.396833  0.000    0.0       False      True   \n","3       1.0  0.019177  0.246042  0.125    0.0        True     False   \n","4       0.5  0.025374  0.346569  0.000    0.0        True     False   \n","..      ...       ...       ...    ...    ...         ...       ...   \n","886     1.0  0.015713  0.258608  0.000    0.0       False      True   \n","887     0.0  0.060508  0.371701  0.000    0.0        True     False   \n","888     1.0  0.015713  0.547625  0.000    0.0       False      True   \n","889     0.0  0.512122  0.258608  0.250    2.0        True     False   \n","890     0.0  0.444099  0.472229  0.000    0.0        True     False   \n","\n","     FemaleFirstClass  ChildFirstClass  IsChild  FamilySize  IsAlone  \n","0                   0                0        0         1.0        1  \n","1                   0                0        1         6.0        0  \n","2                   0                0        0         1.0        1  \n","3                   0                0        0         2.0        0  \n","4                   0                0        0         1.0        1  \n","..                ...              ...      ...         ...      ...  \n","886                 0                0        0         1.0        1  \n","887                 1                0        0         1.0        1  \n","888                 0                0        0         1.0        1  \n","889                 1                0        0         5.0        0  \n","890                 1                0        0         1.0        1  \n","\n","[891 rows x 12 columns]\n"]}],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","training_X = pd.concat([train_X, valid_X, test_X], ignore_index=True) # training_X is the entire training data from train.csv but after all the cleaning, normalization, and feature engineering\n","training_Y = pd.concat([train_Y, valid_Y, test_Y], ignore_index=True) \n","\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","columns_to_normalize = [\"Pclass\", \"Fare\", \"Age\", \"SibSp\"]\n","\n","scaler.fit(training_X[columns_to_normalize])\n","\n","print(training_X)\n","\n","training_X[columns_to_normalize] = scaler.transform(training_X[columns_to_normalize])\n","\n","print(training_X)"]},{"cell_type":"markdown","metadata":{},"source":["# Now that the test data is Normalized and has feature engineering (from previous code), I can use model.predict() on it"]},{"cell_type":"code","execution_count":266,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.013327Z","iopub.status.idle":"2023-12-08T16:54:41.013604Z","shell.execute_reply":"2023-12-08T16:54:41.013486Z","shell.execute_reply.started":"2023-12-08T16:54:41.013473Z"},"trusted":true},"outputs":[],"source":["# Best parameters found:\n","#  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (11, 15), 'max_iter': 4000}\n","# Best score (CV mean): 0.8282828282828284\n","\n","# Commented out the code so that the model won't run everytime\n","\n","# model = MLPClassifier(activation = \"relu\", alpha = 0.0001, hidden_layer_sizes = (11, 15), solver = \"adam\", max_iter=4000, random_state = 42)\n","\n","# model.fit(training_X, training_Y) #Fitting the model on the entire training data set (including all the sub-sets from train-validation-test split)\n","\n","# test_data = test_data.drop(\"PassengerId\", axis = 1)\n","\n","# predictions4 = model.predict(test_data)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Submitting Model 4 (accuracy: 79.425)"]},{"cell_type":"code","execution_count":267,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.014442Z","iopub.status.idle":"2023-12-08T16:54:41.014729Z","shell.execute_reply":"2023-12-08T16:54:41.014605Z","shell.execute_reply.started":"2023-12-08T16:54:41.014589Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data[\"PassengerId\"] = pass_id\n","\n","# predictions4 = predictions4.astype(int)\n","\n","# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions4})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data = test_data.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["# MAKE SURE THERE ISN'T ANY NEGATIVE VALUES FOR AGE BEFORE AND AFTER NORMALIZATION"]},{"cell_type":"code","execution_count":268,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.016090Z","iopub.status.idle":"2023-12-08T16:54:41.016376Z","shell.execute_reply":"2023-12-08T16:54:41.016254Z","shell.execute_reply.started":"2023-12-08T16:54:41.016240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Empty DataFrame\n","Columns: [Pclass, Fare, Age, SibSp, Parch, Sex_female, Sex_male, FemaleFirstClass, ChildFirstClass, IsChild, FamilySize, IsAlone]\n","Index: []\n"]}],"source":["negatives = training_X[training_X[\"Age\"] < 0]\n","print(negatives)\n","\n","# No values negative values for age"]},{"cell_type":"markdown","metadata":{},"source":["# Maybe do GridSearchCV to test only the Alpha value and the max_iter"]},{"cell_type":"markdown","metadata":{},"source":["**TRY THIS OUT SINCE THE PREVIOUS GRIDSEARCH DIDN'T REALLY TRY MANY Alpha VALUES AND DIDN'T TEST OUT DIFFERENT max_iter. Maybe try doing GridSearchCV with different GPU accelerator to see if it goes faster**"]},{"cell_type":"code","execution_count":269,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.017413Z","iopub.status.idle":"2023-12-08T16:54:41.017718Z","shell.execute_reply":"2023-12-08T16:54:41.017600Z","shell.execute_reply.started":"2023-12-08T16:54:41.017586Z"},"trusted":true},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","\n","training_X = pd.concat([train_X, valid_X, test_X], ignore_index=True)\n","training_Y = pd.concat([train_Y, valid_Y, test_Y], ignore_index=True)\n","\n","possible_alpha = []\n","possible_max_iter = []\n","\n","initial_alpha = 0.0001\n","for i in range(8,20):\n","    possible_alpha.append(initial_alpha)\n","    initial_alpha += 0.0005\n","\n","initial_iter = 3500\n","for i in range(8,20):\n","    possible_max_iter.append(initial_iter)\n","    initial_iter += 200\n","\n","\n","parameter_space = {\n","    'alpha': possible_alpha,\n","    \"max_iter\": possible_max_iter\n","}\n","\n","# mlp = MLPClassifier(activation = \"relu\", hidden_layer_sizes = (11, 15), solver = \"adam\", random_state = 42)\n","\n","# clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)\n","\n","# clf.fit(training_X, training_Y)\n","\n","# print('Best parameters found:\\n', clf.best_params_)\n","\n","# print('Best score (CV mean):', clf.best_score_)\n","\n","# print(\"Done\")\n","\n","# Best parameters found:\n","#  {'alpha': 0.0005, 'max_iter': 4000}\n","# Best score (CV mean): 0.8203753687778546"]},{"cell_type":"markdown","metadata":{},"source":["# Testing out Model 5 (accuracy: 78.229)"]},{"cell_type":"code","execution_count":270,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.018741Z","iopub.status.idle":"2023-12-08T16:54:41.019051Z","shell.execute_reply":"2023-12-08T16:54:41.018900Z","shell.execute_reply.started":"2023-12-08T16:54:41.018885Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","\n","# model = MLPClassifier(activation = \"relu\", alpha = 0.0036, hidden_layer_sizes = (15, 15), solver = \"adam\", max_iter=7000, random_state = 42)\n","\n","# model.fit(train_X, train_Y)\n","\n","# y_pred = model.predict(valid_X)\n","\n","# print(f\"Validation Accuracy: {accuracy_score(valid_Y, y_pred)}\")"]},{"cell_type":"code","execution_count":271,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.019832Z","iopub.status.idle":"2023-12-08T16:54:41.020126Z","shell.execute_reply":"2023-12-08T16:54:41.020006Z","shell.execute_reply.started":"2023-12-08T16:54:41.019991Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","\n","# model = MLPClassifier(activation = \"relu\", alpha = 0.0036, hidden_layer_sizes = (15, 15), solver = \"adam\", max_iter=7000, random_state = 42)\n","\n","# model.fit(training_X, training_Y) #Fitting the model on the entire training data set (including all the sub-sets from train-validation-test split)\n","\n","# if \"PassengerId\" in test_data.columns:\n","#     test_data = test_data.drop(\"PassengerId\", axis = 1)\n","\n","# predictions5 = model.predict(test_data)"]},{"cell_type":"code","execution_count":272,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.021107Z","iopub.status.idle":"2023-12-08T16:54:41.021375Z","shell.execute_reply":"2023-12-08T16:54:41.021258Z","shell.execute_reply.started":"2023-12-08T16:54:41.021245Z"},"trusted":true},"outputs":[],"source":["# Commented out the code so that the model won't run everytime\n","\n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data[\"PassengerId\"] = pass_id\n","\n","# predictions5 = predictions5.astype(int)\n","\n","# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions5})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data = test_data.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Make sure to try TensorFlow and other libraries\n","# 2. Learn how to control randomness using the random seeds so that I can replicate the same results using the same code"]},{"cell_type":"code","execution_count":273,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.022305Z","iopub.status.idle":"2023-12-08T16:54:41.022609Z","shell.execute_reply":"2023-12-08T16:54:41.022484Z","shell.execute_reply.started":"2023-12-08T16:54:41.022469Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.13.0\n"]},{"name":"stdout","output_type":"stream","text":["6/6 [==============================] - 0s 0s/step - loss: 0.5080 - accuracy: 0.7978\n","Test Accuracy: 0.7977527976036072\n","6/6 [==============================] - 0s 1ms/step\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from sklearn.model_selection import train_test_split\n","\n","\n","# train_X = train_X.values\n","# valid_X = valid_X.values\n","# train_Y = train__Y.values\n","# valid_Y = valid_Y.values\n","\n","model_tensor = Sequential([\n","    Dense(64, activation='relu', input_shape=(train_X.shape[1],)),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","train_X = train_X.astype('float32')\n","valid_X = valid_X.astype('float32')\n","test_X = test_X.astype('float32')\n","training_X = training_X.astype('float32')\n","training_X2 = training_X.astype('float32')\n","\n","\n","model_tensor.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model_tensor.fit(train_X, train_Y, epochs=100, validation_split=0.2, verbose=0)\n","\n","loss, accuracy = model_tensor.evaluate(valid_X, valid_Y)\n","\n","print(f\"Test Accuracy: {accuracy}\")\n","\n","predictionz = model_tensor.predict(valid_X)\n","#print(predictionz)\n","\n","#print(tf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Submitting Model 6"]},{"cell_type":"code","execution_count":274,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.023524Z","iopub.status.idle":"2023-12-08T16:54:41.023796Z","shell.execute_reply":"2023-12-08T16:54:41.023676Z","shell.execute_reply.started":"2023-12-08T16:54:41.023662Z"},"trusted":true},"outputs":[],"source":["\n","#training_Y = training_Y.astype('float32')\n","test_data2 = test_data.astype('float32')\n","\n","# model_tensor.fit(training_X2, training_Y, epochs=100, validation_split=0.2, verbose=0) #Fitting the model on the entire training data set (including all the sub-sets from train-validation-test split)\n","\n","# # print(training_X.shape)\n","# # print(training_Y.shape)\n","\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# predictions6 = model_tensor.predict(test_data2)\n","# #print(predictions6)"]},{"cell_type":"markdown","metadata":{},"source":["# Change the output predictions to 0 and 1 value. Because the output layer of the neural network is sigmoid, it will give you a value between 0 and 1, but we want binary values so we have to change the output values to 0 and 1"]},{"cell_type":"code","execution_count":275,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.024729Z","iopub.status.idle":"2023-12-08T16:54:41.025022Z","shell.execute_reply":"2023-12-08T16:54:41.024879Z","shell.execute_reply.started":"2023-12-08T16:54:41.024866Z"},"trusted":true},"outputs":[],"source":["# predictions6 = (predictions6 > 0.5).astype(\"int32\")"]},{"cell_type":"markdown","metadata":{},"source":["# Submit the model 6 predictions"]},{"cell_type":"code","execution_count":276,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.025695Z","iopub.status.idle":"2023-12-08T16:54:41.025964Z","shell.execute_reply":"2023-12-08T16:54:41.025836Z","shell.execute_reply.started":"2023-12-08T16:54:41.025823Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(418,)\n","(418,)\n","Your submission was successfully saved!\n","[0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n"," 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0\n"," 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n"," 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1\n"," 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n"," 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n"," 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n"," 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n"," 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0\n"," 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0\n"," 0 0 1 0 1 0 0 1 0 0 1]\n"]}],"source":["original_test = pd.read_csv(\"test.csv\")\n","\n","pass_id = original_test['PassengerId']\n","\n","test_data2[\"PassengerId\"] = pass_id\n","\n","predictions6 = predictions6.astype(int)\n","\n","#Flatten out predictions 6 so that we make sure it's a 1D array\n","\n","predictions6 = predictions6.reshape(-1)\n","\n","print(predictions6.shape)\n","print(test_data.PassengerId.shape)\n","\n","output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions6})\n","output.to_csv('submission.csv', index=False)\n","print(\"Your submission was successfully saved!\")\n","\n","print(predictions6)\n","\n","test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["# Using Keras Tuner, which is TensorFlow's version of GridSearchCV, in order to find the best hyperparamters for my model"]},{"cell_type":"code","execution_count":277,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.027623Z","iopub.status.idle":"2023-12-08T16:54:41.028700Z","shell.execute_reply":"2023-12-08T16:54:41.028469Z","shell.execute_reply.started":"2023-12-08T16:54:41.028439Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reloading Tuner from my_dir\\keras_tuner_demo\\tuner0.json\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"]}],"source":["import keras_tuner as kt\n","\n","\n","\n","def build_model(hp):\n","    model = tf.keras.Sequential()\n","    \n","    model.add(tf.keras.layers.Dense(\n","        units=hp.Int('units', min_value=10, max_value=64, step=4),\n","        activation='relu',\n","        input_shape=(train_X.shape[1],)\n","    ))\n","    \n","    model.add(tf.keras.layers.Dense(\n","        units=hp.Int('units', min_value=10, max_value=64, step=4),\n","        activation='relu'\n","    ))\n","    \n","    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(\n","            hp.Choice('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001])),\n","        loss='binary_crossentropy',\n","        metrics=['accuracy'])\n","    \n","    # model.compile(\n","    # optimizer=tf.keras.optimizers.Adam(\n","    #     learning_rate=hp.Choice('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001]),\n","    #     beta_1=0.9,\n","    #     beta_2=0.999,\n","    #     epsilon=1e-07,\n","    #     amsgrad=False),\n","    # loss='binary_crossentropy',\n","    # metrics=['accuracy'])\n","    \n","    return model\n","\n","tuner = kt.Hyperband(\n","    build_model,\n","    objective='val_accuracy',\n","    max_epochs=100,\n","    directory='my_dir',\n","    project_name='keras_tuner_demo') #MAKE SURE TO CHANGE THE PROJECT NAME TO START A FRESH NEW TUNING SESSION IF YOU WANT TO TRY A NEW SET OF PARAMETERS\n","\n","tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), epochs=100, validation_data=(test_X, test_Y)) \n","#tuner.search(train_X, train_Y, epochs=100, validation_data=(valid_X, valid_Y))\n","\n","# Get the best model\n","best_model = tuner.get_best_models()[0]"]},{"cell_type":"markdown","metadata":{},"source":["# Show information about the model from Keras Tuner"]},{"cell_type":"code","execution_count":278,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.030440Z","iopub.status.idle":"2023-12-08T16:54:41.030829Z","shell.execute_reply":"2023-12-08T16:54:41.030682Z","shell.execute_reply.started":"2023-12-08T16:54:41.030664Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary of Model: \n","WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 10)                130       \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                110       \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 251 (1004.00 Byte)\n","Trainable params: 251 (1004.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Hyperparameters: \n","units 10\n","learning_rate 0.1\n","tuner/epochs 2\n","tuner/initial_epoch 0\n","tuner/bracket 4\n","tuner/round 0\n","Model Configuration: \n","Visualizing the model: \n","Weights and Biases: \n"]}],"source":["#1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","print(\"Summary of Model: \")\n","best_model = tuner.get_best_models(num_models=1)[0]\n","best_model.summary()\n","\n","#2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","print(\"Hyperparameters: \")\n","best_hp = tuner.get_best_hyperparameters()[0]\n","for param in best_hp.values:\n","    print(param, best_hp.get(param))\n","\n","    \n","#3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","print(\"Model Configuration: \")\n","config = best_model.get_config()\n","\n","\n","#4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","print(\"Visualizing the model: \")\n","tf.keras.utils.plot_model(best_model, to_file='model.png', show_shapes=True, show_layer_names=True) # This is downloaded in the kaggle output file\n","\n","\n","#5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","print(\"Weights and Biases: \")\n","# for layer in best_model.layers:\n","#     weights, biases = layer.get_weights()\n","#     print(layer.name, weights, biases)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Testing my new Model (7) from the Keras Tuner"]},{"cell_type":"code","execution_count":279,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.032099Z","iopub.status.idle":"2023-12-08T16:54:41.032467Z","shell.execute_reply":"2023-12-08T16:54:41.032332Z","shell.execute_reply.started":"2023-12-08T16:54:41.032315Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6/6 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.8101\n","Test Accuracy: 0.8100558519363403\n"]}],"source":["model7 = best_model #it's already had .fit() ran on it (inside Keras Tuner) so I don't have to do it \n","\n","test_X = test_X.astype('float32')\n","\n","loss, accuracy = model_tensor.evaluate(test_X, test_Y)\n","\n","print(f\"Test Accuracy: {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Submitting model 7"]},{"cell_type":"code","execution_count":280,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.033567Z","iopub.status.idle":"2023-12-08T16:54:41.033879Z","shell.execute_reply":"2023-12-08T16:54:41.033754Z","shell.execute_reply.started":"2023-12-08T16:54:41.033740Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["14/14 [==============================] - 0s 883us/step\n","Your submission was successfully saved!\n"]}],"source":["# Get the best hyperparameters\n","best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# Rebuild the model with the best hyperparameters\n","model7 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","#model7 = best_model\n","\n","# Fit the model on the entire training dataset\n","\n","model7.fit(training_X2, training_Y, epochs=200, validation_split=0.3, verbose=0)\n","\n","if \"PassengerId\" in test_data2.columns:\n","    test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","predictions7 = model7.predict(test_data2)\n","\n","predictions7 = (predictions7 > 0.6).astype(\"int32\")\n","\n","original_test = pd.read_csv(\"test.csv\")\n","\n","pass_id = original_test['PassengerId']\n","\n","test_data2[\"PassengerId\"] = pass_id\n","\n","#Flatten out predictions so that we make sure it's a 1D array\n","\n","predictions7 = predictions7.reshape(-1)\n","\n","output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions7})\n","output.to_csv('submission.csv', index=False)\n","print(\"Your submission was successfully saved!\")\n","\n","test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["# Changing the Keras Tuner to allow for more parameters because it's so fast"]},{"cell_type":"code","execution_count":281,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.034900Z","iopub.status.idle":"2023-12-08T16:54:41.035252Z","shell.execute_reply":"2023-12-08T16:54:41.035124Z","shell.execute_reply.started":"2023-12-08T16:54:41.035108Z"},"trusted":true},"outputs":[],"source":["# import kerastuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('units', min_value=10, max_value=64, step=1),\n","#         activation='relu',\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('units', min_value=10, max_value=64, step=1),\n","#         activation='relu'\n","#     ))\n","    \n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     model.compile(\n","#         optimizer=tf.keras.optimizers.Adam(\n","#             hp.Choice('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001])),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=10,\n","#     directory='my_dir',\n","#     project_name='keras_tuner_demo')\n","\n","# When you do kerastuner, make sure to change the \"project name\" attribute and maybe even the \"directory\" attribute since this indicates that you are making a fresh new keras tuner\n","\n","# tuner.search(train_X, train_Y, epochs=100, validation_data=(valid_X, valid_Y))\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]"]},{"cell_type":"markdown","metadata":{},"source":["# When you do kerastuner, make sure to change the \"project name\" attribute and maybe even the \"directory\" attribute since this indicates that you are making a fresh new keras tuner"]},{"cell_type":"markdown","metadata":{},"source":["# 0.  Some of the thing aren't working in vscode so make sure to check each box to look at the error messages. For example, the keras_tuner and the visualization from keras tuner\n","\n","# 1. Another thing i want to try is seeing if removing some of the feature enginering will help, since I removed some in the randomforestclassifier and it improved the model\n","\n","# 2. Another thing to try is changing the threshhold from 0.5 to something higher since there were people who died so I want less predictions that say people survived (which is 1 in binary) so I will do this by increasing the threshold. In order words, I'm only saying that someone survived if my model is very sure that they survived (if the number from the sigmoid is high like 0.7).\n","\n","# 2. Another thing to look into is the kernel_initializer since those are how the model finds the initial value for the weights in each layer so look into that\n","\n","# 3. Submit the same code on another notebook to see if I successfully made it so that my code will get the same score if I submit the same code (no more randomness in my model)\n","\n","# 4. Try Keras Tuner with the Bayesian Optimizer instead of hyperband\n","\n","# 5. Earlier, I use a random forest classifier to fill in the missing data for whether someone was a male or female. Maybe try optimizing that or just removing the rows of data for which gender is missing. Maybe do the same for the other columns with missing values"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
