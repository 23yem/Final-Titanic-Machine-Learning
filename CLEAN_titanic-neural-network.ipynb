{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-08T16:54:37.331383Z","iopub.status.busy":"2023-12-08T16:54:37.330932Z","iopub.status.idle":"2023-12-08T16:54:37.342250Z","shell.execute_reply":"2023-12-08T16:54:37.341093Z","shell.execute_reply.started":"2023-12-08T16:54:37.331345Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","print(\"Tensorflow:\", tf.__version__)\n","\n","import kerastuner as kt\n","print(\"kerastuner:\", kt.__version__)\n","\n","import keras_tuner as kt2\n","print(\"keras_tuner:\", kt2.__version__)\n","\n","import platform\n","print(\"Python:\", platform.python_version())\n","\n","import numpy as np\n","print(\"numpy:\", np.__version__)\n","\n","import pandas as pd\n","print(\"pandas:\", pd.__version__)\n","\n","import sklearn\n","print(\"sklearn version:\", sklearn.__version__)\n","\n","import sklearn\n","print(\"sklearn path:\", sklearn.__path__)\n","\n","import matplotlib\n","print(\"matplotlib:\", matplotlib.__version__)\n","\n","import seaborn as sns\n","print(\"seaborn:\", sns.__version__)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Set Global random seed to make sure we can replicate any model that we create (no randomness)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.345037Z","iopub.status.busy":"2023-12-08T16:54:37.344445Z","iopub.status.idle":"2023-12-08T16:54:37.359350Z","shell.execute_reply":"2023-12-08T16:54:37.357656Z","shell.execute_reply.started":"2023-12-08T16:54:37.344996Z"},"trusted":true},"outputs":[],"source":["import random\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","\n","\n","np.random.seed(42)\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'"]},{"cell_type":"markdown","metadata":{},"source":["## **Load the Data from the Database**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.367577Z","iopub.status.busy":"2023-12-08T16:54:37.367107Z","iopub.status.idle":"2023-12-08T16:54:37.396537Z","shell.execute_reply":"2023-12-08T16:54:37.395481Z","shell.execute_reply.started":"2023-12-08T16:54:37.367545Z"},"trusted":true},"outputs":[],"source":["#Remember to eventually clean up the wrong or missing values\n","train_data = pd.read_csv(\"train.csv\")\n","\n","#train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.397932Z","iopub.status.busy":"2023-12-08T16:54:37.397600Z","iopub.status.idle":"2023-12-08T16:54:37.418582Z","shell.execute_reply":"2023-12-08T16:54:37.417408Z","shell.execute_reply.started":"2023-12-08T16:54:37.397901Z"},"trusted":true},"outputs":[],"source":["#Remember to eventually clean up the wrong or missing values\n","test_data = pd.read_csv(\"test.csv\")\n","\n","#test_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Set the target/label (y) values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.454688Z","iopub.status.busy":"2023-12-08T16:54:37.453671Z","iopub.status.idle":"2023-12-08T16:54:37.462318Z","shell.execute_reply":"2023-12-08T16:54:37.461084Z","shell.execute_reply.started":"2023-12-08T16:54:37.454632Z"},"trusted":true},"outputs":[],"source":["#Y_train = pd.get_dummies(train_data[\"Survived\"])\n","Y_train = train_data[\"Survived\"]\n","\n","#Y_train.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Now I will re-initialize my data set and clean up my data before using pd.get_dummies()"]},{"cell_type":"markdown","metadata":{},"source":["**I set up X_train and Y_train, but this time, instead of doing pd.get_Dummies() on X_train, I'm not going to use pd.get_Dummies() until after I clean up the data**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.570822Z","iopub.status.busy":"2023-12-08T16:54:37.570221Z","iopub.status.idle":"2023-12-08T16:54:37.589769Z","shell.execute_reply":"2023-12-08T16:54:37.588219Z","shell.execute_reply.started":"2023-12-08T16:54:37.570788Z"},"trusted":true},"outputs":[],"source":["train_data = pd.read_csv(\"train.csv\")\n","\n","features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"] # Here, I also added more features which I think might be useful\n","\n","\n","# print(train_data[\"Age\"].isnull().sum())\n","# train_data = train_data.dropna(subset=[\"Age\"]) # Drop the rows with missing values of age.\n","# print(train_data[\"Age\"].isnull().sum())\n","\n","# train_data = train_data.dropna(subset=[\"Embarked\"]) # Drop the rows with missing values of Embarked\n","# print(train_data[\"Embarked\"].isnull().sum())\n","\n","X_train = train_data # I won't actually limit the X_train to just the columns of the features I want just yet. This is because I will use the other columns in the data set to help impute missing values\n","\n","Y_train = train_data[\"Survived\"]\n","\n","#print(X_train.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["## Start cleaning up the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T16:54:37.592301Z","iopub.status.busy":"2023-12-08T16:54:37.591487Z","iopub.status.idle":"2023-12-08T16:54:37.604267Z","shell.execute_reply":"2023-12-08T16:54:37.602156Z","shell.execute_reply.started":"2023-12-08T16:54:37.592251Z"},"trusted":true},"outputs":[],"source":["# Impute missing values for Pclass\n","\n","average_Pclass = X_train[\"Pclass\"].median()\n","\n","X_train[\"Pclass\"].fillna(average_Pclass, inplace = True)\n","\n","print(X_train[\"Pclass\"].isnull().sum()) # print the number of rows in the \"Pclass\" columns that now have no value\n","\n","print(X_train[\"Sex\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Sex\" column so we don't need to impute anything\n","\n","print(X_train[\"Fare\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Fare\" column so we don't need to impute anything\n","\n","print(X_train[\"SibSp\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"SibSp\" column so we don't need to impute anything\n","\n","print(X_train[\"Parch\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Parch\" column so we don't need to impute anything\n","\n","print(X_train[\"Age\"].isnull().sum()) # This prints out 177 so there are 177 rows with missing values in the \"Age\" column! This means we have to do a lot of imputing\n","\n","\n","#print(X_train[\"Embarked\"].isnull().sum())\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**From this, we learned that age is the main column with missing values (of the columns that we are working with) so we need to methodically impute those missing values**"]},{"cell_type":"markdown","metadata":{},"source":["## Cleaning up (imputing) the Age column using a Random Forest Regressor model (a type of Machine Learning model) that accurately predicts the age of a person given their information\n"]},{"cell_type":"markdown","metadata":{},"source":["I made it so that when ever you run this code, it will check to see if there is already imputed data (inside of the csv) and if there is, then the imputer won't run since it takes a long time and if the imputed already exists (from previous runs), then there's no point in running the imputer again (which is what I did before and it wasted so much time)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import LabelEncoder\n","\n","imputed_csv_path = 'X_train_imputed.csv'\n","\n","if not os.path.exists(imputed_csv_path):\n","    # If CSV doesn't exist, run the imputation and save the results\n","   #We use \"LabelEncoder\" to turn all the categorial values (values which don't use numbers) into values which the Random Forest Regressor can understand more easily. This is very similar to doing something like pd.get_dummies()\n","\n","    label_encoders = {}\n","    for column in ['Sex', 'Embarked']:\n","        label_encoders[column] = LabelEncoder()\n","        # We use 'astype(str)' to convert any NaN values to a string representation\n","        X_train[column] = label_encoders[column].fit_transform(X_train[column].astype(str))\n","\n","    features = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","    # One very important thing that I will end up doing is using the \"Survived\" column of the dataset to predict the age. This is because there is a strong correleation between the survival and age. \n","\n","\n","    #This is an imputer. It is a function that can efficiently impute values in the way you want. For this scenario, I am using IterativeImputer() and the RandomForestRegressor(() to )\n","    imputer = IterativeImputer(RandomForestRegressor(n_estimators=10), max_iter=100, random_state=42)\n","\n","\n","    # Fit the imputer on the DataFrame with the features\n","    imputer.fit(X_train[features]) #When IterativeImputer is used alongside an ML model like RandomForestRegressor, calling .fit() on the IterativeImputer() object will also fit (train) the ML model\n","\n","\n","    # Perform the imputation on the Training Data. THIS WILL ACTUAL IMPUTE ANY MISSING VALUES IN EACH ROW, not only the missing values in Age.\n","    X_train_imputed = imputer.transform(X_train[features])\n","\n","\n","    # Convert the output back to a DataFrame\n","    X_train_imputed = pd.DataFrame(X_train_imputed, columns=features)\n","\n","    # Update the original DataFrame with the imputed values. This means that the columns of the original X_train dataframe that weren't part of the features (like \"Name\" isn't apart of the features list)\n","    X_train[features] = X_train_imputed\n","\n","    #print(X_train.head())\n","\n","\n","    # Save the imputed DataFrame to CSV\n","    X_train.to_csv(imputed_csv_path, index=False)\n","    print(\"I did imputation again\")\n","\n","else:\n","    # Load the imputed data from the CSV file\n","    X_train = pd.read_csv(imputed_csv_path)\n","    print(\"I used the data from the csv\")\n"," "]},{"cell_type":"markdown","metadata":{},"source":["**Using the ImperativeImputer(), this will actually impute all the missing values in each row. It won't only impute the missing values in the \"age\" column. BUT, since we already cleaned the data and know there are 0 missing values for all the other columns that we care about (the ones with the features we are using), the imputer will be imputing values only to the missing values of age and other columns that we don't care about, but out of all the columns we care about, it will only be affecting the age column since all the other columns we care about have 0 missing values as we checked in the code previously.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.966132Z","iopub.status.idle":"2023-12-08T16:54:40.966690Z","shell.execute_reply":"2023-12-08T16:54:40.966400Z","shell.execute_reply.started":"2023-12-08T16:54:40.966377Z"},"trusted":true},"outputs":[],"source":["features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n","\n","X_train = X_train[features]\n","\n","#print(X_train.head(5))\n","\n","#print((X_train[\"Sex\"] == 0).sum())\n","#print((X_train[\"Sex\"] == 1).sum())\n","\n","# Since the dataset is 65% male, we can see from printing out the number of people with \"sex equal to 0\" and \"sex equal to 1\" that the LabelEncoder made Male into \"1\" and Female into \"0\". Now, let's change it back to \"Male\" and \"Female\".\n","\n","\n","# Check if 'Sex' column contains numeric values\n","if np.issubdtype(X_train['Sex'].dtype, np.number):\n","    # If 'Sex' column is numeric, map 0 to 'female' and 1 to 'male'\n","    X_train['Sex'] = X_train['Sex'].map({0: 'female', 1: 'male'})\n","\n","    \n","\n","if \"Embarked\" in features:\n","    X_train[\"Embarked\"] = X_train[\"Embarked\"].astype(\"category\")\n","    X_train[\"Embarked\"] = X_train[\"Embarked\"].cat.codes\n","\n","print(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["**Here, in the code above, I changed X_train back to focus only on the features we care about. I also changed the \"Sex\" column from 0 and 1 back to Male and Female.**"]},{"cell_type":"markdown","metadata":{},"source":["## After the careful inspection of the test.csv, I see that there are also missing values in the test.csv. As a result, we have to clean up the data for test.csv as we did for train.csv"]},{"cell_type":"markdown","metadata":{},"source":["**Seeing which columns have missing values:**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.968796Z","iopub.status.idle":"2023-12-08T16:54:40.969752Z","shell.execute_reply":"2023-12-08T16:54:40.969564Z","shell.execute_reply.started":"2023-12-08T16:54:40.969535Z"},"trusted":true},"outputs":[],"source":["test_data = pd.read_csv(\"test.csv\")\n","\n","features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n","\n","print(test_data[\"Pclass\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"Sex\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"SibSp\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"Parch\"].isnull().sum()) # Since this is 0, that means there is no missing values\n","\n","print(test_data[\"Fare\"].isnull().sum()) # Since this is 1, that means there is one missing value\n","\n","print(test_data[\"Age\"].isnull().sum()) # Since this is 86, that means there are 86 missing values\n"]},{"cell_type":"markdown","metadata":{},"source":["**From this, we can see that only \"Fare\" annd \"Age\" have missing values. Since \"Fare\" only has one missing value, it's not worth it to go to through all the work to do some advanced imputation technique to fill in missing values. So, I will just fill that in with the average fare value.**\n","\n","**For the \"Age\" feature, I will repeat the process of using an IterativeImputer and a RandomForestRegressor to imputate missing values**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.970708Z","iopub.status.idle":"2023-12-08T16:54:40.971089Z","shell.execute_reply":"2023-12-08T16:54:40.970926Z","shell.execute_reply.started":"2023-12-08T16:54:40.970909Z"},"trusted":true},"outputs":[],"source":["average_fare = test_data[\"Fare\"].mean()\n","\n","test_data[\"Fare\"].fillna(average_fare, inplace = True) # This replaces the single missing value in \"Fare\" with the average value Fare Value of the test data\n","\n","print(test_data[\"Fare\"].isnull().sum())\n","\n","print(test_data[\"Age\"].isnull().sum())"]},{"cell_type":"markdown","metadata":{},"source":["I made it so that when ever you run this code, it will check to see if there is already imputed data (inside of the csv) and if there is, then the imputer won't run since it takes a long time and if the imputed already exists (from previous runs), then there's no point in running the imputer again (which is what I did before and it wasted so much time)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Your imputation code setup here...\n","\n","# Path to the imputed CSV file\n","imputed_csv_path = 'test_data_imputed.csv'\n","\n","# Check if the imputed data CSV file exists\n","if not os.path.exists(imputed_csv_path):\n","    # If CSV doesn't exist, run the imputation and save the results\n","    #We use \"LabelEncoder\" to turn all the categorial values (values which don't use numbers) into values which the Random Forest Regressor can understand more easily. This is very similar to doing something like pd.get_dummies()\n","\n","    label_encoders = {}\n","    for column in ['Sex', 'Embarked']:\n","        label_encoders[column] = LabelEncoder()\n","        # We use 'astype(str)' to convert any NaN values to a string representation\n","        test_data[column] = label_encoders[column].fit_transform(test_data[column].astype(str))\n","\n","    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","\n","\n","    #This is an imputer. It is a function that can efficiently impute values in the way you want. For this scenario, I am using IterativeImputer() and the RandomForestRegressor(() to )\n","    imputer = IterativeImputer(RandomForestRegressor(n_estimators=10), max_iter=100, random_state=42)\n","\n","\n","    # Fit the imputer on the DataFrame with the features\n","    imputer.fit(test_data[features]) #When IterativeImputer is used alongside an ML model like RandomForestRegressor, calling .fit() on the IterativeImputer() object will also fit (train) the ML model\n","\n","\n","    # Perform the imputation on the Test Data. THIS WILL ACTUAL IMPUTE ANY MISSING VALUES IN EACH ROW, not only the missing values in Age.\n","    test_data_imputed = imputer.transform(test_data[features])\n","\n","\n","    # Convert the output back to a DataFrame\n","    test_data_imputed = pd.DataFrame(test_data_imputed, columns=features)\n","\n","    # Update the original DataFrame with the imputed values. This means that the columns of the original test_data dataframe that weren't part of the features (like \"Name\" isn't apart of the features list)\n","    test_data[features] = test_data_imputed\n","\n","    print(test_data[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful \n","\n","    print(test_data.head())\n","\n","    # Save the imputed DataFrame to CSV\n","    test_data.to_csv(imputed_csv_path, index=False)\n","\n","    print(\"I created a new CSV file with imputation\")\n","\n","\n","else:\n","    # Load the imputed data from the CSV file\n","    test_data = pd.read_csv(imputed_csv_path)\n","\n","    print(\"I used imputated data from previous csv\")\n","\n","print(test_data[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful \n"]},{"cell_type":"markdown","metadata":{},"source":["**The code above will imputate all the missing values in the test_data**"]},{"cell_type":"markdown","metadata":{},"source":["## Finalize the cleaning of the data and make it so test_data only has the features we want"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.976046Z","iopub.status.idle":"2023-12-08T16:54:40.977148Z","shell.execute_reply":"2023-12-08T16:54:40.976862Z","shell.execute_reply.started":"2023-12-08T16:54:40.976837Z"},"trusted":true},"outputs":[],"source":["features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n","\n","test_data = test_data[features]\n","\n","#print((test_data[\"Sex\"] == 0).sum())\n","#print((test_data[\"Sex\"] == 1).sum())\n","\n","# Since the dataset is 65% male, we can see from printing out the number of people with \"sex equal to 0\" and \"sex equal to 1\" that the LabelEncoder made Male into \"1\" and Female into \"0\". Now, let's change it back to \"male\" and \"female\".\n","\n","test_data['Sex'] = test_data['Sex'].map({0: 'female', 1: 'male'})\n","\n","if \"Embarked\" in features:\n","    test_data[\"Embarked\"] = test_data[\"Embarked\"].astype(\"category\")\n","    test_data[\"Embarked\"] = test_data[\"Embarked\"].cat.codes\n","\n","\n","print(test_data)\n","\n","#print((test_data[\"Sex\"] == \"male\").sum())"]},{"cell_type":"markdown","metadata":{},"source":["## Now, after cleaning up the data, I can split up the data into train_validation_test "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.978124Z","iopub.status.idle":"2023-12-08T16:54:40.978609Z","shell.execute_reply":"2023-12-08T16:54:40.978469Z","shell.execute_reply.started":"2023-12-08T16:54:40.978453Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","#Y_train = train_data[\"Survived\"]\n","\n","# First, split into training (60%) and a test set (40%)\n","train_X, test_X, train_Y, test_Y = train_test_split(X_train, Y_train, test_size=0.4, random_state = 42)\n","\n","#Second, split test set into a validation (20%) and test set (20%)\n","valid_X, test_X, valid_Y, test_Y= train_test_split(test_X, test_Y, test_size = 0.5, random_state = 42)"]},{"cell_type":"markdown","metadata":{},"source":["## Using pd.get_dummies() on all the data"]},{"cell_type":"markdown","metadata":{},"source":["**Setting up the Test data (from Test.csv) (This is NOT the test data from the Train_validation_test split):**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.980483Z","iopub.status.idle":"2023-12-08T16:54:40.981008Z","shell.execute_reply":"2023-12-08T16:54:40.980791Z","shell.execute_reply.started":"2023-12-08T16:54:40.980767Z"},"trusted":true},"outputs":[],"source":["test_data = pd.get_dummies(test_data)\n","#print(test_data.head())"]},{"cell_type":"markdown","metadata":{},"source":["**Setting up the Train data and Validation data and the Test data from the train_validation_test split:**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:40.982767Z","iopub.status.idle":"2023-12-08T16:54:40.983157Z","shell.execute_reply":"2023-12-08T16:54:40.983015Z","shell.execute_reply.started":"2023-12-08T16:54:40.982993Z"},"trusted":true},"outputs":[],"source":["train_X = pd.get_dummies(train_X)\n","#print(train_X.head())\n","\n","valid_X = pd.get_dummies(valid_X)\n","#print(valid_X.head())\n","\n","test_X = pd.get_dummies(test_X)\n","#print(test_X.head())"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Engineering: Putting more weight onto specific features (like Gender and Age)"]},{"cell_type":"markdown","metadata":{},"source":["**We know from history that Women and Children were more likely to survive so it makes more sense to put more weight into those features. In other words, we should penalize a person more if they are not a Women or Child and therefore make it less likely for them to survive**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.002983Z","iopub.status.idle":"2023-12-08T16:54:41.003288Z","shell.execute_reply":"2023-12-08T16:54:41.003161Z","shell.execute_reply.started":"2023-12-08T16:54:41.003147Z"},"trusted":true},"outputs":[],"source":["#print(train_X)\n","\n","child_age = 18 # Age 7: 0.03, 0.16, Age 8: 0.03, 0.15. Age 9: 0.03, 0.14. Age 10: 0.03, 0.10. Age 16: 0.08, 0.11. \n","\n","#Create a FemaleFirstClass and ChildFirstClass feature into each dataset\n","train_X['FemaleFirstClass'] = ((train_X['Sex_female'] == True) & (train_X['Pclass'] == 1)).astype(int)\n","#train_X['ChildFirstClass'] = ((train_X['Age'] < child_age) & (train_X['Pclass'] == 1)).astype(int) \n","\n","\n","valid_X['FemaleFirstClass'] = ((valid_X['Sex_female'] == True) & (valid_X['Pclass'] == 1)).astype(int)\n","#valid_X['ChildFirstClass'] = ((valid_X['Age'] < child_age) & (valid_X['Pclass'] == 1)).astype(int) \n","\n","\n","test_X['FemaleFirstClass'] = ((test_X['Sex_female'] == True) & (test_X['Pclass'] == 1)).astype(int)\n","#test_X['ChildFirstClass'] = ((test_X['Age'] < child_age) & (test_X['Pclass'] == 1)).astype(int) \n","\n","\n","test_data['FemaleFirstClass'] = ((test_data['Sex_female'] == True) & (test_data['Pclass'] == 1)).astype(int)\n","#test_data['ChildFirstClass'] = ((test_data['Age'] < child_age) & (test_data['Pclass'] == 1)).astype(int) \n","\n","\n","# Bin 'Age' into categories\n","# train_X['IsChild'] = (train_X['Age'] < child_age).astype(int)\n","# valid_X['IsChild'] = (valid_X['Age'] < child_age).astype(int)\n","# test_X['IsChild'] = (test_X['Age'] < child_age).astype(int)\n","# test_data['IsChild'] = (test_data['Age'] < child_age).astype(int)\n","\n","\n","#Create FamilySize and IsAlone\n","train_X['FamilySize'] = train_X['SibSp'] + train_X['Parch'] + 1\n","valid_X['FamilySize'] = valid_X['SibSp'] + valid_X['Parch'] + 1\n","test_X['FamilySize'] = test_X['SibSp'] + test_X['Parch'] + 1\n","test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n","\n","\n","train_X['IsAlone'] = (train_X['FamilySize'] == 1).astype(int)\n","valid_X['IsAlone'] = (valid_X['FamilySize'] == 1).astype(int)\n","test_X['IsAlone'] = (test_X['FamilySize'] == 1).astype(int)\n","test_data['IsAlone'] = (test_data['FamilySize'] == 1).astype(int)\n","\n","\n","#print(train_X)"]},{"cell_type":"markdown","metadata":{},"source":["## Normalizing all the data"]},{"cell_type":"markdown","metadata":{},"source":["**Creating the Normalizing Scaler**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.003933Z","iopub.status.idle":"2023-12-08T16:54:41.004243Z","shell.execute_reply":"2023-12-08T16:54:41.004123Z","shell.execute_reply.started":"2023-12-08T16:54:41.004109Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Select columns you want to normalize\n","columns_to_normalize = [\"Pclass\", \"Fare\",\"Age\", \"SibSp\"]\n","\n","scaler.fit(train_X[columns_to_normalize]) #it is important to fit the scaler on the training data (from the train-validation-test split) only\n","\n","# print(train_X)"]},{"cell_type":"markdown","metadata":{},"source":["**It is very important to only fit the scaler on the training data from the train-validation-test split and not the training data from the whole train.csv since the whole training data includes data from the validation and test split and fitting on that will cause the testing (using validation and test) of the model to be overly optimistic. This is called data leakage.**\n","\n","**Also, it's important that we do not fit the scaler multiple times since that would cause inconsistant fitting throughout the data. It should only be fitted once and then applied to all the different datasets.**\n","\n","**However,before I submit Model 4, I should re-fit the normalization scaler on the entire training data since I will be running my model on the entire training data**"]},{"cell_type":"markdown","metadata":{},"source":["**Normalizing the test data (the one from test.csv)**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.006515Z","iopub.status.idle":"2023-12-08T16:54:41.006902Z","shell.execute_reply":"2023-12-08T16:54:41.006740Z","shell.execute_reply.started":"2023-12-08T16:54:41.006721Z"},"trusted":true},"outputs":[],"source":["test_data = test_data #the \"test_data\" variable here is after we cleaned up the test_data \n","\n","#print(test_data)\n","\n","# Fit the scaler on the data and then transform it\n","test_data[columns_to_normalize] = scaler.transform(test_data[columns_to_normalize])\n","\n","#print(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["**Normalizing the train, validation, and test data**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.007767Z","iopub.status.idle":"2023-12-08T16:54:41.008134Z","shell.execute_reply":"2023-12-08T16:54:41.007988Z","shell.execute_reply.started":"2023-12-08T16:54:41.007938Z"},"trusted":true},"outputs":[],"source":["train_X[columns_to_normalize] = scaler.transform(train_X[columns_to_normalize])\n","#print(train_X.head())\n","\n","valid_X[columns_to_normalize] = scaler.transform(valid_X[columns_to_normalize])\n","#print(valid_X.head())\n","\n","test_X[columns_to_normalize] = scaler.transform(test_X[columns_to_normalize])\n","#print(test_X.head())\n","\n","#print(train_X)"]},{"cell_type":"markdown","metadata":{},"source":["## I should re-fit the normalization scaler on the entire training data since I will be running my model on the entire training data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.012335Z","iopub.status.idle":"2023-12-08T16:54:41.012615Z","shell.execute_reply":"2023-12-08T16:54:41.012495Z","shell.execute_reply.started":"2023-12-08T16:54:41.012481Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","training_X = pd.concat([train_X, valid_X, test_X], ignore_index=True) # training_X is the entire training data from train.csv but after all the cleaning, normalization, and feature engineering\n","training_Y = pd.concat([train_Y, valid_Y, test_Y], ignore_index=True) \n","\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","columns_to_normalize = [\"Pclass\", \"Fare\", \"Age\", \"SibSp\"]\n","\n","scaler.fit(training_X[columns_to_normalize])\n","\n","#print(training_X)\n","\n","training_X[columns_to_normalize] = scaler.transform(training_X[columns_to_normalize])\n","\n","#print(training_X)"]},{"cell_type":"markdown","metadata":{},"source":["## MAKE SURE THERE ISN'T ANY NEGATIVE VALUES FOR AGE BEFORE AND AFTER NORMALIZATION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.016090Z","iopub.status.idle":"2023-12-08T16:54:41.016376Z","shell.execute_reply":"2023-12-08T16:54:41.016254Z","shell.execute_reply.started":"2023-12-08T16:54:41.016240Z"},"trusted":true},"outputs":[],"source":["negatives = training_X[training_X[\"Age\"] < 0]\n","#print(negatives)\n","\n","# No values negative values for age"]},{"cell_type":"markdown","metadata":{},"source":["## Try TensorFlow Neural Network Model"]},{"cell_type":"markdown","metadata":{},"source":["**Change all the data to float32 so that it can be converted into tensor**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_X = train_X.astype('float32')\n","valid_X = valid_X.astype('float32')\n","test_X = test_X.astype('float32')\n","training_X = training_X.astype('float32')\n","training_X2 = training_X.astype('float32')\n","\n","test_data2 = test_data.astype('float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.022305Z","iopub.status.idle":"2023-12-08T16:54:41.022609Z","shell.execute_reply":"2023-12-08T16:54:41.022484Z","shell.execute_reply.started":"2023-12-08T16:54:41.022469Z"},"trusted":true},"outputs":[],"source":["# import tensorflow as tf\n","# print(tf.__version__)\n","\n","# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Dense, Dropout\n","# from sklearn.model_selection import train_test_split\n","\n","\n","# # train_X = train_X.values\n","# # valid_X = valid_X.values\n","# # train_Y = train__Y.values\n","# # valid_Y = valid_Y.values\n","\n","\n","# model_tensor = Sequential([\n","#     Dense(64, activation='relu', input_shape=(train_X.shape[1],)),\n","#     Dense(32, activation='relu'),\n","#     Dense(1, activation='sigmoid')\n","# ])\n","\n","\n","# model_tensor.compile(optimizer='adam',\n","#               loss='binary_crossentropy',\n","#               metrics=['accuracy'])\n","\n","# history = model_tensor.fit(train_X, train_Y, epochs=100, validation_split=0.2, verbose=0)\n","\n","# loss, accuracy = model_tensor.evaluate(valid_X, valid_Y)\n","\n","# print(f\"Test Accuracy: {accuracy}\")\n","\n","# predictionz = model_tensor.predict(valid_X)\n","#print(predictionz)\n","\n","#print(tf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Using Keras Tuner, which is TensorFlow's version of GridSearchCV, in order to find the best hyperparamters for my model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.027623Z","iopub.status.idle":"2023-12-08T16:54:41.028700Z","shell.execute_reply":"2023-12-08T16:54:41.028469Z","shell.execute_reply.started":"2023-12-08T16:54:41.028439Z"},"trusted":true},"outputs":[],"source":["# import keras_tuner as kt\n","\n","\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('units', min_value=10, max_value=64, step=4),\n","#         activation='relu',\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('units', min_value=10, max_value=64, step=4),\n","#         activation='relu'\n","#     ))\n","    \n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     model.compile(\n","#         optimizer=tf.keras.optimizers.Adam(\n","#             hp.Choice('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001])),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     # model.compile(\n","#     # optimizer=tf.keras.optimizers.Adam(\n","#     #     learning_rate=hp.Choice('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001]),\n","#     #     beta_1=0.9,\n","#     #     beta_2=0.999,\n","#     #     epsilon=1e-07,\n","#     #     amsgrad=False),\n","#     # loss='binary_crossentropy',\n","#     # metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     directory='my_dir',\n","#     project_name='keras_tuner_demo') #MAKE SURE TO CHANGE THE PROJECT NAME TO START A FRESH NEW TUNING SESSION IF YOU WANT TO TRY A NEW SET OF PARAMETERS\n","\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), epochs=100, validation_data=(test_X, test_Y)) \n","# #tuner.search(train_X, train_Y, epochs=100, validation_data=(valid_X, valid_Y))\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]"]},{"cell_type":"markdown","metadata":{},"source":["## Show information about the model from Keras Tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.030440Z","iopub.status.idle":"2023-12-08T16:54:41.030829Z","shell.execute_reply":"2023-12-08T16:54:41.030682Z","shell.execute_reply.started":"2023-12-08T16:54:41.030664Z"},"trusted":true},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")\n","# tf.keras.utils.plot_model(best_model, to_file='model.png', show_shapes=True, show_layer_names=True) # This is downloaded in the kaggle output file\n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Testing my new Model (7) from the Keras Tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:54:41.032099Z","iopub.status.idle":"2023-12-08T16:54:41.032467Z","shell.execute_reply":"2023-12-08T16:54:41.032332Z","shell.execute_reply.started":"2023-12-08T16:54:41.032315Z"},"trusted":true},"outputs":[],"source":["# model7 = best_model #it's already had .fit() ran on it (inside Keras Tuner) so I don't have to do it \n","\n","# test_X = test_X.astype('float32')\n","\n","# loss, accuracy = model_tensor.evaluate(test_X, test_Y) #This won't actually work because the test data was used in the keras tuner \n","\n","# print(f\"Test Accuracy: {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting model 7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# model7 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #model7 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# model7.fit(training_X2, training_Y, epochs=200, validation_split=0.3, verbose=0)\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# predictions7 = model7.predict(test_data2)\n","\n","# predictions7 = (predictions7 > 0.6).astype(\"int32\")\n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions7 = predictions7.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions7})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["## Changing the Keras Tuner to allow for more parameters because it's so fast"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import keras_tuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     # Input layer\n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('input_units', min_value=5, max_value=64, step=1),\n","#         activation=hp.Choice('input_activation', ['relu', 'tanh', 'sigmoid']),\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     # Hidden layers\n","#     for i in range(hp.Int('n_layers', 1, 5)):\n","#         model.add(tf.keras.layers.Dense(\n","#             units=hp.Int(f'hidden_units_{i}', min_value=5, max_value=64, step=1),\n","#             activation=hp.Choice(f'hidden_activation_{i}', ['relu', 'tanh', 'sigmoid']),\n","#             kernel_regularizer=tf.keras.regularizers.l2(hp.Float(f'regularization_{i}', min_value=0.0, max_value=0.1, step=0.01))\n","#         ))\n","    \n","#     # Output layer\n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     # Compile the model\n","#     model.compile(\n","#         optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     directory='directory_for_model_8',\n","#     project_name='keras_tuner_model8',\n","#     hyperband_iterations=2)  # Increase if you have more time\n","\n","# # Search for the best hyperparameters\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), \n","#              epochs=100, \n","#              validation_data=(test_X, test_Y), \n","#              batch_size=32)  # Fixed batch size\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]\n","\n","# # This model took 508 trials and almost 10 minutes to run\n","\n","#Hyper parameters:\n","# input_units 21\n","# input_activation relu\n","# n_layers 3\n","# hidden_units_0 11\n","# hidden_activation_0 tanh\n","# regularization_0 0.1\n","# optimizer adam\n","# hidden_units_1 59\n","# hidden_activation_1 relu\n","# regularization_1 0.0\n","# hidden_units_2 11\n","# hidden_activation_2 tanh\n","# regularization_2 0.0\n","# hidden_units_3 20\n","# hidden_activation_3 relu\n","# regularization_3 0.06\n","# hidden_units_4 33\n","# hidden_activation_4 tanh\n","# regularization_4 0.09\n","# tuner/epochs 100\n","# tuner/initial_epoch 34\n","# tuner/bracket 3\n","# tuner/round 3\n","# tuner/trial_id 0461"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# #config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")\n","# tf.keras.utils.plot_model(best_model, to_file='model.png', show_shapes=True, show_layer_names=True) # This is downloaded in the kaggle output file\n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model8 = best_model #it's already had .fit() ran on it (inside Keras Tuner) so I don't have to do it \n","\n","# test_X = test_X.astype('float32')\n","\n","# loss, accuracy = model_tensor.evaluate(test_X, test_Y)\n","\n","# print(f\"Test Accuracy: {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting model 8 (Score: 78.8)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# #model8 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #Or, another thing you can do is just use the best model from the keras tuner instead of re-fitting it on the full training data\n","# model8 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# #model8.fit(training_X2, training_Y, epochs=200, validation_split=0.2, verbose=0)\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# predictions8 = model8.predict(test_data2)\n","\n","# predictions8 = (predictions8 > 0.5).astype(\"int32\")\n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions8 = predictions8.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions8})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["## Change the keras tuner even more and add validation split and drop out to make sure it doesn't over fit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import keras_tuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     # Input layer\n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('input_units', min_value=5, max_value=64, step=1),\n","#         activation=hp.Choice('input_activation', ['relu', 'tanh', 'sigmoid']),\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     # Hidden layers\n","#     for i in range(hp.Int('n_layers', 1, 5)):\n","#         model.add(tf.keras.layers.Dense(\n","#             units=hp.Int(f'hidden_units_{i}', min_value=5, max_value=64, step=1),\n","#             activation=hp.Choice(f'hidden_activation_{i}', ['relu', 'tanh', 'sigmoid']),\n","#             kernel_regularizer=tf.keras.regularizers.l2(hp.Float(f'regularization_{i}', min_value=0.0, max_value=0.1, step=0.01))\n","#         ))\n","#         # Dropout layer\n","#         model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n","    \n","#     # Output layer\n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     # Compile the model\n","#     model.compile(\n","#         optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     directory='directory_for_model_9',  #Make sure to change ethe directory  and project name if you want to start a fresh new tuning session\n","#     project_name='keras_tuner_model9', # make sure to change the project and directory name if you want to start a fresh new tuning session\n","#     hyperband_iterations=2)  # Increase if you have more time\n","\n","# # Search for the best hyperparameters\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), \n","#              epochs=100, \n","#              validation_split=0.2,  # 20% of the data will be used for validation\n","#              batch_size=32)  # Fixed batch size\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]\n","\n","# #This took 12 minutes for the code to run. 507 trials\n","\n","# # input_units 54\n","# # input_activation relu\n","# # n_layers 3\n","# # hidden_units_0 30\n","# # hidden_activation_0 relu\n","# # regularization_0 0.1\n","# # dropout_rate 0.4\n","# # optimizer rmsprop\n","# # hidden_units_1 49\n","# # hidden_activation_1 relu\n","# # regularization_1 0.0\n","# # hidden_units_2 58\n","# # hidden_activation_2 relu\n","# # regularization_2 0.0\n","# # hidden_units_3 60\n","# # hidden_activation_3 relu\n","# # regularization_3 0.03\n","# # hidden_units_4 43\n","# # hidden_activation_4 sigmoid\n","# # regularization_4 0.06\n","# # tuner/epochs 100\n","# # tuner/initial_epoch 34\n","# # tuner/bracket 2\n","# # tuner/round 2\n","# # tuner/trial_id 0484\n","# # Model Configuration: \n","# # Visualizing the model: \n","# # Weights and Biases: \n","# # Optimizer: \n","# # {'name': 'RMSprop', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': 100, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.001, 'rho': 0.9, 'momentum': 0.0, 'epsilon': 1e-07, 'centered': False}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")  # MAKE\n","# tf.keras.utils.plot_model(best_model, to_file='model9.png', show_shapes=True, show_layer_names=True) # MAKE SURE TO CHANGE THE FILE NAME \n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","# #6 see the best optimizer specifically \n","# print(\"Optimizer: \")\n","# # Get the optimizer's configuration\n","# optimizer_config = best_model.optimizer.get_config()\n","\n","# # Print the optimizer configuration\n","# print(optimizer_config)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting Model 9 (validation split and drop out, along with more search space in keras tuner). Score: 78.708"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# #model9 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #Or, another thing you can do is just use the best model from the keras tuner instead of re-fitting it on the full training data\n","# model9 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# #model9.fit(training_X2, training_Y, epochs=100, validation_split=0.2, verbose=0)\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# predictions9 = model9.predict(test_data2)\n","\n","# predictions9 = (predictions9 > 0.72).astype(\"int32\") \n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions9 = predictions9.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions9})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["## Try removing the rows of data with missing age and then redoing keras tuner"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import keras_tuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     # Input layer\n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('input_units', min_value=5, max_value=64, step=1),\n","#         activation=hp.Choice('input_activation', ['relu', 'tanh', 'sigmoid']),\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     # Hidden layers\n","#     for i in range(hp.Int('n_layers', 1, 5)):\n","#         model.add(tf.keras.layers.Dense(\n","#             units=hp.Int(f'hidden_units_{i}', min_value=5, max_value=64, step=1),\n","#             activation=hp.Choice(f'hidden_activation_{i}', ['relu', 'tanh', 'sigmoid']),\n","#             kernel_regularizer=tf.keras.regularizers.l2(hp.Float(f'regularization_{i}', min_value=0.0, max_value=0.1, step=0.01))\n","#         ))\n","#         # Dropout layer\n","#         model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n","    \n","#     # Output layer\n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     # Compile the model\n","#     model.compile(\n","#         optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     directory='directory_for_model_10',  #Make sure to change ethe directory  and project name if you want to start a fresh new tuning session\n","#     project_name='keras_tuner_model10', # make sure to change the project and directory name if you want to start a fresh new tuning session\n","#     hyperband_iterations=2)  # Increase if you have more time\n","\n","# # Search for the best hyperparameters\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), \n","#              epochs=100, \n","#              validation_split=0.2,  # 20% of the data will be used for validation\n","#              batch_size=32)  # Fixed batch size\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]\n","\n","# # Took 10 minutes and 38 seconds\n","\n","# # Hyperparameters: \n","# # input_units 31\n","# # input_activation relu\n","# # n_layers 1\n","# # hidden_units_0 16\n","# # hidden_activation_0 tanh\n","# # regularization_0 0.0\n","# # dropout_rate 0.1\n","# # optimizer adam\n","# # hidden_units_1 49\n","# # hidden_activation_1 relu\n","# # regularization_1 0.09\n","# # hidden_units_2 36\n","# # hidden_activation_2 relu\n","# # regularization_2 0.04\n","# # hidden_units_3 26\n","# # hidden_activation_3 sigmoid\n","# # regularization_3 0.09\n","# # hidden_units_4 15\n","# # hidden_activation_4 sigmoid\n","# # regularization_4 0.06\n","# # tuner/epochs 100\n","# # tuner/initial_epoch 34\n","# # tuner/bracket 3\n","# # tuner/round 3\n","# # tuner/trial_id 0206\n","# # Model Configuration: \n","# # Visualizing the model: \n","# # Weights and Biases: \n","# # Optimizer: \n","# # {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")  # MAKE\n","# tf.keras.utils.plot_model(best_model, to_file='model10.png', show_shapes=True, show_layer_names=True) # MAKE SURE TO CHANGE THE FILE NAME \n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","# #6 see the best optimizer specifically \n","# print(\"Optimizer: \")\n","# # Get the optimizer's configuration\n","# optimizer_config = best_model.optimizer.get_config()\n","\n","# # Print the optimizer configuration\n","# print(optimizer_config)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting model 10 (the model that deleted the training data with missing age). Score: 77.5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# #model10 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #Or, another thing you can do is just use the best model from the keras tuner instead of re-fitting it on the full training data\n","# model10 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# #model10.fit(training_X2, training_Y, epochs=100, validation_split=0.2, verbose=0)\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# predictions = model10.predict(test_data2)\n","\n","# predictions = (predictions > 0.65).astype(\"int32\") \n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions = predictions.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["## For model 11, I will try removing some feature engineering and redo keras tuner on this new data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import keras_tuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     # Input layer\n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('input_units', min_value=5, max_value=64, step=1),\n","#         activation=hp.Choice('input_activation', ['relu', 'tanh', 'sigmoid']),\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     # Hidden layers\n","#     for i in range(hp.Int('n_layers', 1, 5)):\n","#         model.add(tf.keras.layers.Dense(\n","#             units=hp.Int(f'hidden_units_{i}', min_value=5, max_value=64, step=1),\n","#             activation=hp.Choice(f'hidden_activation_{i}', ['relu', 'tanh', 'sigmoid']),\n","#             kernel_regularizer=tf.keras.regularizers.l2(hp.Float(f'regularization_{i}', min_value=0.0, max_value=0.1, step=0.01))\n","#         ))\n","#         # Dropout layer\n","#         model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n","    \n","#     # Output layer\n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     # Compile the model\n","#     model.compile(\n","#         optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     directory='directory_for_model_11',  #Make sure to change ethe directory  and project name if you want to start a fresh new tuning session\n","#     project_name='keras_tuner_model11', # make sure to change the project and directory name if you want to start a fresh new tuning session\n","#     hyperband_iterations=2)  # Increase if you have more time\n","\n","# # Search for the best hyperparameters\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), \n","#              epochs=100, \n","#              validation_split=0.2,  # 20% of the data will be used for validation\n","#              batch_size=32)  # Fixed batch size\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]\n","\n","# # Hyperparameters: \n","# # input_units 31\n","# # input_activation relu\n","# # n_layers 1\n","# # hidden_units_0 16\n","# # hidden_activation_0 tanh\n","# # regularization_0 0.0\n","# # dropout_rate 0.1\n","# # optimizer adam\n","# # hidden_units_1 49\n","# # hidden_activation_1 relu\n","# # regularization_1 0.09\n","# # hidden_units_2 36\n","# # hidden_activation_2 relu\n","# # regularization_2 0.04\n","# # hidden_units_3 26\n","# # hidden_activation_3 sigmoid\n","# # regularization_3 0.09\n","# # hidden_units_4 15\n","# # hidden_activation_4 sigmoid\n","# # regularization_4 0.06\n","# # tuner/epochs 100\n","# # tuner/initial_epoch 34\n","# # tuner/bracket 3\n","# # tuner/round 3\n","# # tuner/trial_id 0204\n","# # Model Configuration: \n","# # Visualizing the model: \n","# # Weights and Biases: \n","# # Optimizer: \n","# # {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")  # MAKE\n","# tf.keras.utils.plot_model(best_model, to_file='model11.png', show_shapes=True, show_layer_names=True) # MAKE SURE TO CHANGE THE FILE NAME \n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","# #6 see the best optimizer specifically \n","# print(\"Optimizer: \")\n","# # Get the optimizer's configuration\n","# optimizer_config = best_model.optimizer.get_config()\n","\n","# # Print the optimizer configuration\n","# print(optimizer_config)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting Model 11 (removing some feature engineering). Score: 78.229"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# #model11 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #Or, another thing you can do is just use the best model from the keras tuner instead of re-fitting it on the full training data\n","# model11 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# #model11.fit(training_X2, training_Y, epochs=100, validation_split=0.2, verbose=0)\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# predictions = model11.predict(test_data2)\n","\n","# predictions = (predictions > 0.6).astype(\"int32\") \n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions = predictions.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["## For model 12, I want to try to add back some of the feature engineering that I removed for Model 11"]},{"cell_type":"markdown","metadata":{},"source":["**I only included 'FemaleFirstClass', 'FamilySize', 'IsAlone'. Also the threshhold for being a child is \"18.\" So if you want to replicate this result, make sure to change it back to 18**"]},{"cell_type":"markdown","metadata":{},"source":["**Originally, I ran this code with TensorFlow version 2.13.0, but it seem like version 2.15.0 gets the same results, but probably because it's using the previous keras tuner trials. To be safe, use Tensor Flow 2.13.0**\n","\n","**And here are the other versions:**\n","- **tensorflow: 2.13.0**\n","- **Kerastuner: 1.0.5**\n","- **keras_tuner: 1.3.5, I THINK. If not, maybe try 1.4.6**\n","- **Python: 3.10.11**\n","- **numpy: 1.24.3**\n","- **pandas: 2.1.4, I THINK. If not, try 2.0.3**\n","- **sklearn version: 1.2.2**\n","- **sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import keras_tuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     # Input layer\n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('input_units', min_value=5, max_value=64, step=1),\n","#         activation=hp.Choice('input_activation', ['relu', 'tanh', 'sigmoid']),\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     # Hidden layers\n","#     for i in range(hp.Int('n_layers', 1, 5)):\n","#         model.add(tf.keras.layers.Dense(\n","#             units=hp.Int(f'hidden_units_{i}', min_value=5, max_value=64, step=1),\n","#             activation=hp.Choice(f'hidden_activation_{i}', ['relu', 'tanh', 'sigmoid']),\n","#             kernel_regularizer=tf.keras.regularizers.l2(hp.Float(f'regularization_{i}', min_value=0.0, max_value=0.1, step=0.01))\n","#         ))\n","#         # Dropout layer\n","#         model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n","    \n","#     # Output layer\n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     # Compile the model\n","#     model.compile(\n","#         optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     directory='directory_for_model_12_new',  #Make sure to change ethe directory  and project name if you want to start a fresh new tuning session\n","#     project_name='keras_tuner_model12_new', # make sure to change the project and directory name if you want to start a fresh new tuning session\n","#     hyperband_iterations=2)  # Increase if you have more time\n","\n","# # Search for the best hyperparameters\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), \n","#              epochs=100, \n","#              validation_split=0.2,  # 20% of the data will be used for validation\n","#              batch_size=32)  # Fixed batch size\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]\n","\n","\n","# # _________________________________________________________________\n","# #  Layer (type)                Output Shape              Param #   \n","# # =================================================================\n","# #  dense (Dense)               (None, 50)                550       \n","                                                                 \n","# #  dense_1 (Dense)             (None, 33)                1683      \n","                                                                 \n","# #  dropout (Dropout)           (None, 33)                0         \n","                                                                 \n","# #  dense_2 (Dense)             (None, 48)                1632      \n","                                                                 \n","# #  dropout_1 (Dropout)         (None, 48)                0         \n","                                                                 \n","# #  dense_3 (Dense)             (None, 1)                 49        \n","                                                                 \n","# # =================================================================\n","# # Total params: 3914 (15.29 KB)\n","# # Trainable params: 3914 (15.29 KB)\n","# # Non-trainable params: 0 (0.00 Byte)\n","# # _________________________________________________________________\n","\n","# #Hyperparameters: \n","# # input_units 50\n","# # input_activation tanh\n","# # n_layers 2\n","# # hidden_units_0 33\n","# # hidden_activation_0 relu\n","# # regularization_0 0.0\n","# # dropout_rate 0.4\n","# # optimizer rmsprop\n","# # hidden_units_1 48\n","# # hidden_activation_1 tanh\n","# # regularization_1 0.05\n","# # hidden_units_2 23\n","# # hidden_activation_2 relu\n","# # regularization_2 0.01\n","# # hidden_units_3 37\n","# # hidden_activation_3 tanh\n","# # regularization_3 0.1\n","# # hidden_units_4 46\n","# # hidden_activation_4 tanh\n","# # regularization_4 0.02\n","# # tuner/epochs 100\n","# # tuner/initial_epoch 34\n","# # tuner/bracket 1\n","# # tuner/round 1\n","# # tuner/trial_id 0498\n","# # Model Configuration: \n","# # Visualizing the model: \n","# # Weights and Biases: \n","# # Optimizer: \n","# # {'name': 'RMSprop', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': 100, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.001, 'rho': 0.9, 'momentum': 0.0, 'epsilon': 1e-07, 'centered': False}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")  # MAKE\n","# tf.keras.utils.plot_model(best_model, to_file='model12.png', show_shapes=True, show_layer_names=True) # MAKE SURE TO CHANGE THE FILE NAME \n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","# #6 see the best optimizer specifically \n","# print(\"Optimizer: \")\n","# # Get the optimizer's configuration\n","# optimizer_config = best_model.optimizer.get_config()\n","\n","# # Print the optimizer configuration\n","# print(optimizer_config)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting Model 12. Score: 80.662!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from tensorflow.keras.models import load_model\n","\n","# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# model12 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #Or, another thing you can do is just use the best model from the keras tuner instead of re-fitting it on the full training data \n","# #model12 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# model12.fit(training_X2, training_Y, epochs=115, validation_split=0.2, verbose=0) # Before, I used to do epoch of 100 here, but I should do a slightly higher epoch since I am testing on more data (entire training dataset). I found for here, 115 is good\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# #model12.save(\"model_12_saved.h5\") # This is saving the model in an H5 file. This makes sure that I never lose the model with these weights and everything else\n","\n","# #model12 = load_model(\"model_12_saved.h5\")\n","\n","# predictions = model12.predict(test_data2)\n","\n","# predictions = (predictions > 0.5).astype(\"int32\") # 0.5 and 0.52 got a score of 79.425. For rebuilt model, I got an 80.143 using 0.52 and 0.54 and 0.545. With epoch 115, I got 80.662 with 0.5\n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions = predictions.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["## Using different techniques to find the best features to have, including a correlation matrix and heatmap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","full_data = pd.concat([training_X2, training_Y], axis = 1, ignore_index=False) #This is all the data (including the features and labels)\n","\n","#Making a correlation matrix\n","corr_matrix = full_data.corr()\n","\n","#print(full_data)\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n","\n","# Save the figure\n","plt.savefig('heatmap3.png')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**From the correlation Matrix and heatmap, I think I want to remove FamilySize, and just let SibSp and Parch do it's thing. I also want to add ChildFirstClass and isChild**"]},{"cell_type":"markdown","metadata":{},"source":["## SelectKBest is a good tool to see what features are good to include"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest, f_classif\n","\n","\n","# Create the SelectKBest with the f_classif function. You can set the parameter \"k\" equal to a number if you want to limit the amount of features\n","selector = SelectKBest(f_classif)\n","\n","# Fit the selector to the data\n","selector.fit(training_X2, training_Y)\n","\n","# Get the boolean mask of the selected features\n","mask = selector.get_support()\n","\n","# Get the names of the selected features\n","selected_features = training_X.columns[mask]\n","\n","print(selected_features)\n","\n","\n","\n","# Get the scores\n","scores = selector.scores_\n","\n","# Create a DataFrame with the scores\n","features_scores = pd.DataFrame({'Feature': training_X2.columns, 'Score': scores})\n","\n","# Sort the DataFrame by score in descending order\n","features_scores = features_scores.sort_values(by='Score', ascending=False)\n","\n","print(features_scores)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Creating age bins using matplotlib and finding out which young ages are more likely to survive "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv(\"train.csv\")\n","\n","# Assuming 'df' is your DataFrame and it has columns 'Age' and 'Survived'\n","# Create age ranges\n","\n","bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80]\n","labels = ['0-5', '5-10', '10-15', '15-20', '20-25', '25-30', '30-35', '35-40', '40-45', '45-50', '50-55', '55-60', '60-65', '65-70', '70-75', '75-80']\n","df['AgeRange'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n","\n","# Calculate mean survival rate for each age range\n","age_survival = df.groupby('AgeRange')['Survived'].mean()\n","\n","# Plot\n","age_survival.plot(kind='bar', figsize=(10, 6))\n","plt.ylabel('Survival Rate')\n","plt.title('Survival Rate by Age Range')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv(\"train.csv\")\n","\n","# Create age ranges of 1 year for ages 0-22\n","bins = list(range(23))\n","labels = [str(i) for i in range(22)]\n","df['AgeRange'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n","\n","# Calculate mean survival rate for each age range\n","age_survival = df.groupby('AgeRange')['Survived'].mean()\n","\n","# Plot\n","age_survival.plot(kind='bar', figsize=(10, 6))\n","plt.ylabel('Survival Rate')\n","plt.title('Survival Rate by Age Range')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv(\"train.csv\")\n","\n","# Create age ranges of 1 year for ages 0-22\n","bins = list(range(23))\n","labels = [str(i) for i in range(22)]\n","df['AgeRange'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n","\n","# Calculate mean survival rate for each age range\n","age_survival = df.groupby('AgeRange')['Survived'].mean()\n","\n","# Calculate count of people in each age range\n","age_counts = df['AgeRange'].value_counts().sort_index()\n","\n","# Plot\n","fig, ax1 = plt.subplots(figsize=(10, 6))\n","\n","color = 'tab:blue'\n","ax1.set_xlabel('Age Range')\n","ax1.set_ylabel('Survival Rate', color=color)\n","ax1.bar(age_survival.index, age_survival, color=color)\n","ax1.tick_params(axis='y', labelcolor=color)\n","\n","ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","\n","color = 'tab:red'\n","ax2.set_ylabel('Number of People', color=color)  # we already handled the x-label with ax1\n","ax2.plot(age_counts.index, age_counts, color=color)\n","ax2.tick_params(axis='y', labelcolor=color)\n","\n","fig.tight_layout()  # otherwise the right y-label is slightly clipped\n","plt.title('Survival Rate and Number of People by Age Range')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## For model 13, I'm changing the threshhold for being a child from 18 to 15. I also want to add ChildFirstClass and isChild "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import keras_tuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     # Input layer\n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('input_units', min_value=1, max_value=64, step=1),\n","#         activation=hp.Choice('input_activation', ['relu', 'tanh', 'sigmoid']),\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     # Hidden layers\n","#     for i in range(hp.Int('n_layers', 1, 5)):\n","#         model.add(tf.keras.layers.Dense(\n","#             units=hp.Int(f'hidden_units_{i}', min_value=1, max_value=64, step=1),\n","#             activation=hp.Choice(f'hidden_activation_{i}', ['relu', 'tanh', 'sigmoid']),\n","#             kernel_regularizer=tf.keras.regularizers.l2(hp.Float(f'regularization_{i}', min_value=0.0, max_value=0.1, step=0.01))\n","#         ))\n","#         # Dropout layer\n","#         model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n","    \n","#     # Output layer\n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     # Compile the model\n","#     model.compile(\n","#         optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     directory='directory_for_model_13',  #Make sure to change ethe directory  and project name if you want to start a fresh new tuning session\n","#     project_name='keras_tuner_model13', # make sure to change the project and directory name if you want to start a fresh new tuning session\n","#     hyperband_iterations=2)  # Increase if you have more time\n","\n","# # Search for the best hyperparameters\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), \n","#              epochs=100, \n","#              validation_split=0.2,  # 20% of the data will be used for validation\n","#              batch_size=32)  # Fixed batch size\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]\n","\n","# #This took 13 minutes\n","\n","\n","# # Hyperparameters: \n","# # input_units 53\n","# # input_activation relu\n","# # n_layers 1\n","# # hidden_units_0 59\n","# # hidden_activation_0 tanh\n","# # regularization_0 0.01\n","# # dropout_rate 0.1\n","# # optimizer rmsprop\n","# # hidden_units_1 37\n","# # hidden_activation_1 relu\n","# # regularization_1 0.06\n","# # hidden_units_2 35\n","# # hidden_activation_2 sigmoid\n","# # regularization_2 0.01\n","# # hidden_units_3 63\n","# # hidden_activation_3 relu\n","# # regularization_3 0.04\n","# # hidden_units_4 11\n","# # hidden_activation_4 sigmoid\n","# # regularization_4 0.1\n","# # tuner/epochs 100\n","# # tuner/initial_epoch 34\n","# # tuner/bracket 3\n","# # tuner/round 3\n","# # tuner/trial_id 0205\n","# # Model Configuration: \n","# # Visualizing the model: \n","# # Weights and Biases: \n","# # Optimizer: \n","# # {'name': 'RMSprop', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': 100, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.001, 'rho': 0.9, 'momentum': 0.0, 'epsilon': 1e-07, 'centered': False}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")  # MAKE\n","# tf.keras.utils.plot_model(best_model, to_file='model13.png', show_shapes=True, show_layer_names=True) # MAKE SURE TO CHANGE THE FILE NAME \n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","# #6 see the best optimizer specifically \n","# print(\"Optimizer: \")\n","# # Get the optimizer's configuration\n","# optimizer_config = best_model.optimizer.get_config()\n","\n","# # Print the optimizer configuration\n","# print(optimizer_config)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting Model 13. Score: 76"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# model13 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #Or, another thing you can do is just use the best model from the keras tuner instead of re-fitting it on the full training data \n","# #model13 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# model13.fit(training_X2, training_Y, epochs=115, validation_split=0.2, verbose=0) # Before, I used to do epoch of 100 here, but I should do a slightly higher epoch since I am testing on more data (entire training dataset). I found for here, 115 is good\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# model13.save(\"model_13_saved.h5\") # This is saving the model in an H5 file. This makes sure that I never lose the model with these weights and everything else\n","\n","# predictions = model13.predict(test_data2)\n","\n","# predictions = (predictions > 0.5).astype(\"int32\") # 0.5 and 0.52 got a score of 79.425. For rebuilt model, I got an 80.143 using 0.52 and 0.54 and 0.545. With epoch 115, I got 80.662 with 0.5\n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions = predictions.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"markdown","metadata":{},"source":["## For model 14, I want to do the exact same thing as Model 12 (my best model yet), but I want to expand the range for the number of possible neurons per layer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import keras_tuner as kt\n","\n","# def build_model(hp):\n","#     model = tf.keras.Sequential()\n","    \n","#     # Input layer\n","#     model.add(tf.keras.layers.Dense(\n","#         units=hp.Int('input_units', min_value=1, max_value=70, step=1),\n","#         activation=hp.Choice('input_activation', ['relu', 'tanh', 'sigmoid']),\n","#         input_shape=(train_X.shape[1],)\n","#     ))\n","    \n","#     # Hidden layers\n","#     for i in range(hp.Int('n_layers', 1, 5)):\n","#         model.add(tf.keras.layers.Dense(\n","#             units=hp.Int(f'hidden_units_{i}', min_value=1, max_value=70, step=1),\n","#             activation=hp.Choice(f'hidden_activation_{i}', ['relu', 'tanh', 'sigmoid']),\n","#             kernel_regularizer=tf.keras.regularizers.l2(hp.Float(f'regularization_{i}', min_value=0.0, max_value=0.1, step=0.01))\n","#         ))\n","#         # Dropout layer\n","#         model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n","    \n","#     # Output layer\n","#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","    \n","#     # Compile the model\n","#     model.compile(\n","#         optimizer=hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n","#         loss='binary_crossentropy',\n","#         metrics=['accuracy'])\n","    \n","#     return model\n","\n","# tuner = kt.Hyperband(\n","#     build_model,\n","#     objective='val_accuracy',\n","#     max_epochs=100,\n","#     seed=42,   # MAKE SURE TO SET A SEED SO THAT YOU CONTROL RANDOMNESS\n","#     directory='directory_for_model_14',  #Make sure to change ethe directory  and project name if you want to start a fresh new tuning session\n","#     project_name='keras_tuner_model14', # make sure to change the project and directory name if you want to start a fresh new tuning session\n","#     hyperband_iterations=2)  # Increase if you have more time\n","\n","# # Search for the best hyperparameters\n","# tuner.search(pd.concat([train_X, valid_X], ignore_index=True), pd.concat([train_Y, valid_Y], ignore_index=True), \n","#              epochs=100, \n","#              validation_split=0.2,  # 20% of the data will be used for validation\n","#              batch_size=32)  # Fixed batch size\n","\n","# # Get the best model\n","# best_model = tuner.get_best_models()[0]\n","\n","# # Best val_accuracy So Far: 0.8181818127632141\n","# # Total elapsed time: 00h 17m 48s\n","\n","# # Model: \"sequential\"\n","# # _________________________________________________________________\n","# #  Layer (type)                Output Shape              Param #   \n","# # =================================================================\n","# #  dense (Dense)               (None, 67)                737       \n","                                                                 \n","# #  dense_1 (Dense)             (None, 59)                4012      \n","                                                                 \n","# #  dropout (Dropout)           (None, 59)                0         \n","                                                                 \n","# #  dense_2 (Dense)             (None, 19)                1140      \n","                                                                 \n","# #  dropout_1 (Dropout)         (None, 19)                0         \n","                                                                 \n","# #  dense_3 (Dense)             (None, 47)                940       \n","                                                                 \n","# #  dropout_2 (Dropout)         (None, 47)                0         \n","                                                                 \n","# #  dense_4 (Dense)             (None, 24)                1152      \n","                                                                 \n","# #  dropout_3 (Dropout)         (None, 24)                0         \n","                                                                 \n","# #  dense_5 (Dense)             (None, 52)                1300      \n","                                                                 \n","# #  dropout_4 (Dropout)         (None, 52)                0         \n","                                                                 \n","# #  dense_6 (Dense)             (None, 1)                 53        \n","                                                                 \n","# # =================================================================\n","# # Total params: 9334 (36.46 KB)\n","# # Trainable params: 9334 (36.46 KB)\n","# # Non-trainable params: 0 (0.00 Byte)\n","# # _________________________________________________________________\n","# # Hyperparameters: \n","# # input_units 67\n","# # input_activation relu\n","# # n_layers 5\n","# # hidden_units_0 59\n","# # hidden_activation_0 tanh\n","# # regularization_0 0.0\n","# # dropout_rate 0.0\n","# # optimizer adam\n","# # hidden_units_1 19\n","# # hidden_activation_1 tanh\n","# # regularization_1 0.04\n","# # hidden_units_2 47\n","# # hidden_activation_2 relu\n","# # regularization_2 0.0\n","# # hidden_units_3 24\n","# # hidden_activation_3 tanh\n","# # regularization_3 0.01\n","# # hidden_units_4 52\n","# # hidden_activation_4 tanh\n","# # regularization_4 0.03\n","# # tuner/epochs 100\n","# # tuner/initial_epoch 34\n","# # tuner/bracket 4\n","# # tuner/round 4\n","# # tuner/trial_id 0142\n","# # Model Configuration: \n","# # Visualizing the model: \n","# # Weights and Biases: \n","# # Optimizer: \n","# # {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #1 Summary of the model: This will print a summary of the model's layers, output shapes, and the number of parameters.\n","# print(\"Summary of Model: \")\n","# best_model = tuner.get_best_models(num_models=1)[0]\n","# best_model.summary()\n","\n","# #2 Inspect Hyperparameters: This will display the values of the hyperparameters like the number of units in each layer, learning rate, etc.\n","# print(\"Hyperparameters: \")\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","# for param in best_hp.values:\n","#     print(param, best_hp.get(param))\n","\n","    \n","# #3 Model Configuration: This returns a Python dictionary containing the model configuration. It can be quite detailed and technical, showing layer types, activation functions, and other layer-specific settings.\n","# print(\"Model Configuration: \")\n","# config = best_model.get_config()\n","\n","\n","# #4 Visualizing the Model: This creates a plot of the model architecture, showing the layers, their shapes, and how they're connected.\n","# print(\"Visualizing the model: \")  # MAKE\n","# tf.keras.utils.plot_model(best_model, to_file='model14.png', show_shapes=True, show_layer_names=True) # MAKE SURE TO CHANGE THE FILE NAME \n","\n","\n","# #5 Weights and Biases: This will print out the weights and biases for each layer in the model. Be cautious with large models, as this can be a lot of data.\n","# print(\"Weights and Biases: \")\n","# # for layer in best_model.layers:\n","# #     weights, biases = layer.get_weights()\n","# #     print(layer.name, weights, biases)\n","\n","\n","# #6 see the best optimizer specifically \n","# print(\"Optimizer: \")\n","# # Get the optimizer's configuration\n","# optimizer_config = best_model.optimizer.get_config()\n","\n","# # Print the optimizer configuration\n","# print(optimizer_config)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submitting Model 14. Score 78.229"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from sklearn.model_selection import train_test_split\n","\n","# # Get the best hyperparameters\n","# best_hp = tuner.get_best_hyperparameters()[0]\n","\n","# # Rebuild the model with the best hyperparameters\n","# model14 = build_model(best_hp)   # I have to re-build the model since I have to re-fit on the whole training data (training_X2) \n","\n","# #Or, another thing you can do is just use the best model from the keras tuner instead of re-fitting it on the full training data \n","# #model14 = best_model\n","\n","# # Fit the model on the entire training dataset\n","\n","# model14.fit(training_X2, training_Y, epochs=115, validation_split=0.2, verbose=0) # Before, I used to do epoch of 100 here, but I should do a slightly higher epoch since I am testing on more data (entire training dataset). I found for here, 115 is good\n","\n","# if \"PassengerId\" in test_data2.columns:\n","#     test_data2 = test_data2.drop(\"PassengerId\", axis = 1)\n","\n","# model14.save(\"model_14_saved.h5\") # This is saving the model in an H5 file. This makes sure that I never lose the model with these weights and everything else\n","\n","# predictions = model14.predict(test_data2)\n","\n","# predictions = (predictions > 0.6).astype(\"int32\") # 0.5 and 0.52 got a score of 79.425. For rebuilt model, I got an 80.143 using 0.52 and 0.54 and 0.545. With epoch 115, I got 80.662 with 0.5\n","\n","# original_test = pd.read_csv(\"test.csv\")\n","\n","# pass_id = original_test['PassengerId']\n","\n","# test_data2[\"PassengerId\"] = pass_id\n","\n","# #Flatten out predictions so that we make sure it's a 1D array\n","\n","# predictions = predictions.reshape(-1)\n","\n","# output = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions})\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")\n","\n","# test_data2 = test_data2.drop(\"PassengerId\", axis = 1) # WE make sure to drop the PassengerId column since we are continuing to use the test_data variable in future tests and we don't want PassengerId to be apart of it until we submit the model again"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import tensorflow_decision_forests as tfdf\n","\n","# import tensorflow as tf\n","\n","# print(tf.__version__)\n","# # Load a dataset\n","# train_df = pd.read_csv(\"train.csv\")\n","\n","# # Convert the pandas dataframe to a tensorflow dataset\n","# train_data = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, task=tfdf.keras.Task.CLASSIFICATION)\n","\n","# # Train the model\n","# model = tfdf.keras.RandomForestModel()\n","# model.fit(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["## Recursive feature elimination (from scikit learn) is another great tool to see what the best features are "]},{"cell_type":"markdown","metadata":{},"source":["## Find a way to remove FamilySize without affect isChild. And make sure you don't make it too complicated incase you want to add back FamilySize\n","\n","## Another thing I want to do is try different techniques to find the best features. And this time, I want to include EVERY feature, including the feature engineering"]},{"cell_type":"markdown","metadata":{},"source":["## When you do kerastuner, make sure to change the \"project name\" attribute and maybe even the \"directory\" attribute since this indicates that you are making a fresh new keras tuner"]},{"cell_type":"markdown","metadata":{},"source":["**Play around with the threshold some more and then also try adding/removing some more feature engineering. Also try playing with the epochs on the fitting and also whether to build new model or use the one from keras.**\n","\n","**After that, use advanced techniques to try to find the best features**"]},{"cell_type":"markdown","metadata":{},"source":["**1. Another thing i want to try is seeing if removing some of the feature enginering will help, since I removed some in the randomforestclassifier and it improved the model**\n","\n","**2. Another thing to look into is the kernel_initializer since those are how the model finds the initial value for the weights in each layer so look into that**\n","\n","**4. Try Keras Tuner with the Bayesian Optimizer instead of hyperband**\n","\n","**5. Earlier, I use a random forest classifier to fill in the missing data for whether someone was a male or female. Maybe try optimizing that or just removing the rows of data for which gender is missing. Maybe do the same for the other columns with missing values**\n","\n","**7. Try Random forest classifier on tensor flow. Also try XGBoost**\n","\n","**8. Maybe try optimizing the age imputer using tensorflow neural network or random forest classifier**\n","\n","**9. Try looking into different ways to see which features are best to choose from, like using correlation matrix or Scikit learn SelectKBest**\n","\n","**10. Research more about the pipeline and see if you can use it. Also, research about the XGBoost**"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
